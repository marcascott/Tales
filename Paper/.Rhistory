estTbl[,i] <- estTbl[,i]*sds
}
#update
estTbl.nyc <- estTbl
seTbl.nyc <- seTbl
}
###
dat.sub <- left_join(weeklyVaxDat,dat.all)
minWeek <- min(dat.sub$week)
maxWeek <- max(dat.sub$week)
fit <- vector('list',1+maxWeek-minWeek)
se <- beta <- vector("numeric",1+maxWeek-minWeek)
for (i in minWeek:maxWeek) {
fit[[i-minWeek+1]] <- fit0 <-  glm(formula=count~I(log(pct))+factor(borocode)+offset(log(base)),data=dat.sub %>% filter(week==i),family="quasipoisson")
sd.local <- sd(log(dat.sub$pct[dat.sub$week==i])) #for stdzn
se[i-minWeek+1] <- as.numeric(summary(fit0)$coef["I(log(pct))","Std. Error"])*sd.local
beta[i-minWeek+1] <- fit0$coefficients["I(log(pct))"]*sd.local
}
#append these to existing se, beta for NYC:
rsltList$NYC$seList$logPctVaxd <- rep(NA,length(seList[[1]])) #placeholder
rsltList$NYC$betaList$logPctVaxd <- rep(NA,length(seList[[1]])) #placeholder
rsltList$NYC$seList$logPctVaxd[minWeek:maxWeek] <- se #note these are stdzd
rsltList$NYC$betaList$logPctVaxd[minWeek:maxWeek] <- beta #note these are stdzd
#one viz:
pltDatW <- data.frame(week=minWeek:maxWeek,beta,se)
pltDatW <- pltDatW %>% mutate(lowBd=beta-1.96*se,uppBd=beta+1.96*se)
pltDatL <- reshape::melt(pltDatW,id=c("week")) %>% filter(variable != "se") %>% mutate(bound=grepl("Bd",variable))
gg<-ggplot(data=pltDatL,aes(x=week,y=value,group=variable,linetype=factor(bound)))+geom_line(data=pltDatL,stat="smooth",method = "loess", span=.25,formula=y~x)+ stat_smooth(se=F,span=.25,size=.5)+geom_hline(yintercept=0)+scale_linetype_manual(values=c("solid","dotted"))+ theme(legend.position = "none")+ylab("Standardized Coefficient")+ggtitle("Borough-adjusted effect of (logged) fully vaccinated rate on COVID incidence rate")
#save viz. for full set of plots
#print(gg)
#attempt to viz the curves:
myDat <-subDat %>% group_by(GROUP) %>%summarise(rawrate=sum(count),PercentVaxDose=log(mean(PERC_1PLUS)),borocode=min(borocode),base=min(base)) %>% ungroup %>% mutate(CovidRate=100000*rawrate/base)
gg <- ggplot(myDat, aes(x=PercentVaxDose,y=CovidRate,group=borocode,col=factor(borocode,labels=bnames[ifelse(removeSI,-5,-6)])))+geom_point()+geom_smooth(span=1)+theme(legend.title=element_blank())+ylab("Covid Rate/100000")+xlab("Avg % Fully Vaxxed (logged)")+ggtitle("Feb-Apr 2021 (3 months)")
print(gg)
GAM.RUN<-F
#special run
if (GAM.RUN) {
require(gam); require(mgcv)
fit<-gam(formula=count~s(Essential)+s(week)+I(factor(nbhdcode))+offset(log(base)),data=dat.all,subset=week<20,family="quasipoisson")
plot(fit,main="GAM fit for pooled weeks <20 with s(Essential) + nbhd indicators")
fit<-gam(formula=count~s(Essential)+s(week)+I(factor(nbhdcode))+offset(log(base)),data=dat.all,subset=week>=20&week<35,family="quasipoisson")
plot(fit,main="GAM fit for pooled 20 <= week <35 with s(Essential) + nbhd indicators")
fit<-gam(formula=count~s(Essential)+s(week)+I(factor(nbhdcode))+offset(log(base)),data=dat.all,subset=week>=35,family="quasipoisson")
plot(fit,main="GAM fit for pooled weeks 35+ with s(Essential) + nbhd indicators")
}
MI.M <- 20 # number of imputes
datOrig <- as_tibble(read_stata("../Data/London/NewWeek12t126.dta"))
datCore <- as_tibble(read_stata("../Data/London/NewWeek12t76_imputedFu0.dta")) %>% filter(imputeNumber==1) %>% select(-newCases,-imputeNumber)
#correlation plot
demog.London <- datCore %>% filter(week==15) %>% mutate(GROUP=MSOA11CD,Essential=100*proportion_at_risk_jobs,BAplus=BAp,PercentOlder=100*Age65plus/AllAges,PercentYounger=100*Age015/AllAges,COPD=Asthma,Obese=Obesity18,PercentWhite=100-100*all_bame_prop,PercentHispanic=100*pakistani_or_bangladeshi_prop,PercentBlack=BK,OnlyEngAtHome.acs=100-FoLa,MedianInc=MedInc/1000) %>% mutate(BOROUGH_GROUP=case_when(borocode %in% c("07","20","19","22","28","33")~"Central", borocode %in% c("03","10","14")~"North", borocode %in% c("06","08","21","24","29","32")~"South", borocode %in% c("05", "09", "13","15","17","18","27")~"West", TRUE~"East")) %>% select(GROUP,BOROUGH_GROUP, Essential,BAplus,PercentOlder,PercentYounger, COPD,Obese,OnlyEngAtHome.acs,PercentWhite,PercentHispanic,PercentBlack,MedianInc)
#log as requested
demog.London <- takeLog(demog.London,logVarNames)
names(demog.London) <-  gsub(".acs","",names(demog.London))
st(demog.London,out="kable")
#st(demog.London,out="latex",file="demog.London.table.tex")
ggpairs(data=demog.London,columns=3:13,aes(colour=BOROUGH_GROUP),lower = list(continuous = wrap(ggally_points, size = .1)),upper=list(continuous=wrap(ggally_cor,cex=1.4)),diag = list(continuous = wrap("densityDiag", alpha = 0.5)),title = "Correlation matrix: London")+theme_grey(base_size = 5)
origLastWeek <- max(datCore$week)
newLastWeek <- max(datOrig$week)
datImp <- as_tibble(read_stata("../Data/London/NewWeek12t126_imputed0.dta")) %>% select(LaCode,MSOA11CD,week,newCases,imputeNumber)
datLastWeek <- datCore %>% filter(week==origLastWeek) #one full record
for (i in (1+origLastWeek):newLastWeek) {
datCore <- add_row(datCore, (datLastWeek%>%mutate(week=i)))
}
dat <- datCore %>% mutate(imputeNumber=1) %>% left_join(datImp%>%filter(imputeNumber==1))
dat <- dat %>% add_row(datCore %>% mutate(imputeNumber=2) %>% left_join(datImp%>%filter(imputeNumber==2)))
if (MI.M>2) {
for (i in seq(2,MI.M-2,2)) {
datImp <- as_tibble(read_stata(paste0("../Data/London/NewWeek12t126_imputed",i,".dta"))) %>% select(LaCode,MSOA11CD,week,newCases,imputeNumber)
dat <- dat %>% add_row(datCore %>% mutate(imputeNumber=1+i) %>% left_join(datImp%>%filter(imputeNumber==1+i)))
dat <- dat %>% add_row(datCore %>% mutate(imputeNumber=2+i) %>% left_join(datImp%>%filter(imputeNumber==2+i)))
}
}
dat <- dat %>% mutate(borocode=case_when(borocode %in% c("07","20","19","22","28","33")~1, borocode %in% c("03","10","14")~3, borocode %in% c("06","08","21","24","29","32")~4, borocode %in% c("05", "09", "13","15","17","18","27")~5, TRUE~2), base=AllAges, count=as.integer(newCases), over_65_prop= Age65plus/AllAges, under_16_prop= Age015/AllAges) %>% filter(LaCode!="")
dat$OnlyEngAtHome.acs <- 100-dat$FoLa
dat$PercentWhite <- 1-dat$all_bame_prop
dat$Latinx <- dat$pakistani_or_bangladeshi_prop
#LONDON  - EARLIER to handle MI part
weeklyVaxDat <- as_tibble(haven::read_stata ("../Data/London/vax60-101.dta")) %>% select(MSOA11CD,week,PERC_1PLUS=percent1p) %>% mutate(logPctVaxd=log(PERC_1PLUS)) #weeklyVaxDat
firstVaxWeek <- 53
mnWeek <- min(weeklyVaxDat$week)
singleVaxDat <- as_tibble(weeklyVaxDat) %>% filter(week==mnWeek) %>% select(MSOA11CD,week,PERC_1PLUS)
#use firstVaxDat to interpolate to 2021-01-03 (week 53), when it first was effectively non-0.
for (i in (mnWeek-1):firstVaxWeek) {
weeklyVaxDat <- weeklyVaxDat %>% add_row(singleVaxDat %>% mutate(week=i,logPctVaxd=log(PERC_1PLUS*(1+i-firstVaxWeek)/(1+mnWeek-firstVaxWeek))))
}
firstLondonWeek <- 12
for (i in (firstVaxWeek-1):firstLondonWeek) {
weeklyVaxDat <- weeklyVaxDat %>% add_row(singleVaxDat %>% mutate(week=i,PERC_1PLUS=NA,logPctVaxd=NA))
}
weeklyVaxDat <- weeklyVaxDat %>% select(-PERC_1PLUS) %>% arrange(MSOA11CD,week)
nWeeks.UK <- nWeeks <- sum(!is.na(unique(dat$week))) #toss out the NAs - KLUDGE
varNames.UK <- c("proportion_at_risk_jobs","BAp","over_65_prop","under_16_prop","Asthma", "Obesity18", "OnlyEngAtHome.acs", "PercentWhite", "Latinx","BK", "MedInc") #names of key variables for regression
nVars <- length(varNames.UK)
#create common varnames across datasets - make the proportions percents
for (k in 1:nVars) {
dat[varNames.US[k]] <- dat[varNames.UK[k]]*ifelse(varNames.US[k]%in%c("COPD","BAplus","Obese","OnlyEngAtHome.acs", "PercentBlack","MedianInc"),1,100) # some need *100  - but not all...
}
#adjust MedInc:
dat["MedInc"] <- dat["MedInc"]/1000 #income in 1000s
dat["MedianInc"] <- dat["MedianInc"]/1000 #income in 1000s - should not be needed
#log as requested
dat <- takeLog(dat,logVarNames)
#now we can use a common set of varnames
#initialize lists to store regression runs
#now we can add vaxx:
varNames <- c(varNames,"logPctVaxd") #may need to reset this...
nVars <- nVars+1
dat <- dat %>% left_join(weeklyVaxDat,by=c("MSOA11CD","week"))
seList <- betaList <- vector("list",nVars)
betaList[1:nVars]<-rep(NA,nWeeks)
seList[1:nVars]<-rep(NA,nWeeks)
names(betaList) <- names(seList) <- varNames[1:nVars] #correction to handle lack of COPD in UK data
weekOffset <- 11 #necessary to align the weeks as closely as possible
#no speedup for this part (redoVarNames)... sorry
if (readFromFile %in% c("NYC","NEITHER")) {
cat("Imputing week:")
for (i in 1:nWeeks) {
cat(paste0(i,"."))
dat.last <- dat%>%filter(week==i+weekOffset)
for (k in 1:nVars) {
fmla <- as.formula(paste0("count~",varNames[k],"+factor(LaCode)+offset(log(base))"))
if (MI.M>0) {
if (!all(is.na(dat.last[varNames[k]]))) {
fit <- fitMIqpois(fmla,dat.last,varNames[k],MI.M)
seList[[k]][i] <- fit$se
betaList[[k]][i] <- fit$beta
} else { #should only happen with vaxx
seList[[k]][i] <- NA
betaList[[k]][i] <- NA
}
} else {
fit<-glm(formula=fmla,data=dat.last,family="quasipoisson"  )
seList[[k]][i] <- summary(fit)$coef[varNames[k],"Std. Error"]
betaList[[k]][i] <- fit$coefficients[varNames[k]]
}
}
}
if (rescale) {
for (i in 1:nWeeks) {
dat.last <- dat%>%filter(week==i+weekOffset)
sds <- sapply(dat.last[,varNames],sd)
for (k in 1:nVars) {
seList[[k]][i] <- seList[[k]][i]*sds[k]
betaList[[k]][i] <- betaList[[k]][i]*sds[k]
}
}
}
cat("\n\n")
save(betaList,seList,file="../Data/London/betaList_seList.Rdata")
} else load(file="../Data/London/betaList_seList.Rdata")
dateLabels <- format.Date(as.Date(as.Date("2020-03-22")+7*(0:(nWeeks-1))),"%d%b%y")
#dateLabels <- format.Date(sort(unique(dat$date3)),"%d%b")  #problematic with missing dates
rsltList$LON <- list(betaList=betaList,seList=seList,dateLabels=dateLabels,weekNums=1:nWeeks)
#reset varNames, nVars for next imputes:
varNames <- varNames[1:(length(varNames)-1)]
nVars <- nVars-1
#keyIntervals/nIntervals defined in NYC portion
numCores <- parallel::detectCores()
cl <- parallel::makeCluster(numCores, setup_strategy = "sequential")
registerDoParallel(cl)
names(mlm.fit) <- names(keyIntervals)
estTbl <- matrix(NA,nVars,nIntervals)
seTbl <- matrix(NA,nVars,nIntervals)
dimnames(seTbl) <- dimnames(estTbl) <- list(varNames, names(keyIntervals))
residList.lon <- vector("list",length=nIntervals)
names(residList.lon) <- names(keyIntervals)
fnPrefix <- paste0("est_se_tbl",ifelse(stdModel[2],"","_LAfe"))
if (readFromFile %in% c("NYC","NEITHER")) {
cat(paste0("MI and MLM may take time (in parallel, ~1-2 hours for stdModel, 8x more for LAfe...please be patient\n",date(),'\n'))
mdlCore <- ifelse(stdModel[2],"+factor(borocode)+poly(week,3)+offset(log(base))+(1|LaCode/MSOA11CD)","+factor(LaCode)+poly(week,3)+offset(log(base))+(1|MSOA11CD)") #two different model types
idx <- cbind(rep(1:nIntervals,each=nVars),rep(1:nVars,times=nIntervals))
residList <- vector("list",length=nIntervals*nVars)
names(residList) <- paste0("[",apply(idx,1,function(x) paste0(x,collapse=",")),"]")
#kludge to allow partial re-runs
if (!is.null(redoVarNames)) {
#load data, then limit runs
load(file=paste0("../Data/London/",fnPrefix,".Rdata"))
redoIdx <- match(redoVarNames,varNames)
idx.hold <- idx
idx <- idx[idx[,2]%in%redoIdx,] #subset
#load fits
load(file=paste0("../Debug/fitList_Lon.Rdata"))
fit.hold <- fit #will need to slide in some new fits
}
#repeat looping to exploit foreach
fit <- foreach(i=1:nrow(idx)) %dopar% fitMImepois((fmla<-as.formula(paste0("count~",varNames[idx[i,2]],mdlCore))),subDat <- dat[dat$week>=keyIntervals[[idx[i,1]]][1]+weekOffset & dat$week<=keyIntervals[[idx[i,1]]][2]+weekOffset,],varNames[idx[i,2]],MI.M/mimFactor)
#clean up "fit" list
if (!is.null(redoVarNames)) {
ct <- 1
for (i in 1:nrow(idx.hold)) { #clean up
if (idx.hold[i,2] %in% redoIdx) {
fit.hold[[i]] <- fit[[ct]]
ct <- ct + 1
}
} #now fit.hold is 'correct' wrt new runs
fit <- fit.hold
idx <- idx.hold
}
for (i in 1:nrow(idx)) { #clean up
seTbl[idx[i,2],idx[i,1]] <- fit[[i]]$se
estTbl[idx[i,2],idx[i,1]] <- fit[[i]]$beta
residList[[i]] <- fit[[i]]$resids
}
save(estTbl,seTbl,file=paste0("../Data/London/",fnPrefix,".Rdata"))
save(fit,file=paste0("../Debug/fitList_Lon.Rdata"))
for (i in 1:nIntervals) {
obsDat <- dat[dat$week>=keyIntervals[[i]][1]+weekOffset & dat$week<=keyIntervals[[i]][2]+weekOffset,] %>% filter(imputeNumber==1 & !is.na(count)) %>% select(MSOA11CD, count, week) #there remain a few missings even after impute (probably if totaled count exceeds LA count)
newList <- residList[which(i==idx[,1])]
newList <- lapply(newList,rlist::list.cbind) #still a list
resids <- vector("list",length(newList))
for (k in seq_along(newList)) {
resids[[k]] <- data.frame(newList[[k]])
names(resids[[k]]) <- paste0("Res_Imp",1:(MI.M/mimFactor))
resids[[k]] <- add_column(obsDat,resids[[k]])
}
residList.lon[[i]] <- resids
save(resids,file=paste0("../Data/London/",fnPrefix,"_residsPeriod",i,".Rdata"))
}
stopCluster(cl)
cat(date(),'\n')
} else {
load(file=paste0("../Data/London/",fnPrefix,".Rdata"))
for (i in 1:nIntervals) {
load(file=paste0("../Data/London/",fnPrefix,"_residsPeriod",i,".Rdata"))
residList.lon[[i]] <- resids
names(residList.lon[[i]]) <- varNames
}
}
if (rescale) {
for (i in 1:nIntervals) {
subDat <- dat[dat$week>=keyIntervals[[i]][1]+weekOffset & dat$week<=keyIntervals[[i]][2]+weekOffset,]
sds <- sapply(subDat[,varNames],sd)
seTbl[,i] <- seTbl[,i]*sds
estTbl[,i] <- estTbl[,i]*sds
}
}
#remove after rerunning NYC models
#colnames(estTbl)[colnames(estTbl)=="earlysummer21"] <- "summer21"
#colnames(seTbl)[colnames(seTbl)=="earlysummer21"] <- "summer21"
colnames(estTbl)[colnames(estTbl)=="spring23"] <- "spring22"
colnames(seTbl)[colnames(seTbl)=="spring23"] <- "spring22"
#update
estTbl.lon <- estTbl
seTbl.lon <- seTbl
lmList <-vector("list",nIntervals)
lmList.unwtd <-vector("list",nIntervals)
stdzCoef <- c() #standardized coefficients
intRatio <- c()
pvalSlope <- vector("numeric",nIntervals)
for(i in 1:nIntervals) {
y <- estTbl.lon[,i]
x <- estTbl.nyc[,i]
w <- seTbl.nyc[,i]^2
lmList[[i]] <- lm(y~x,weights=1/w) #WLS, asm London 'precise enough'
lmList.unwtd[[i]] <- lm(y~x) #OLS
stdzCoef[i] <- lmList[[i]]$coefficients["x"]*sd(x)/sd(y) #standardize
int1 <- predict(lmList[[i]],newdata=data.frame(x=0),interval="confidence",level = 0.95)
int2 <- predict(lmList.unwtd[[i]],newdata=data.frame(x=0),interval="confidence",level = 0.95)
intRatio[i] <- diff(int1[1,])[2]/diff(int2[1,])[2]
fstat <- summary(lmList[[i]])$fstatistic
pvalSlope[i] <- 1-pf(fstat[1],fstat[2],fstat[3])
}
zDat <- read_stata("../Data/London/MsoaDist.dta") # MSOA Centroids as x,y
zDat <- zDat %>% rename(lon=x,lat=y,GROUP=MSOA11CD) %>% select(lat,lon,GROUP)
#KLUDGE: if the imputes somehow reduce the MSOA count (happened in t124 run), reduce dist matrix here.
###zDat <- zDat %>% filter(GROUP %in% unique((datOrig%>%filter(LaName!=""))$MSOA11CD)) # due to problem with impute and data t124 that had to have 66 MSOAs removed!
spatTestList <- vector("list",nIntervals)
names(spatTestList) <- names(keyIntervals)
for (i in 1:nIntervals) {
spatTestList[[i]] <- vector("list",nVars)
names(spatTestList[[i]]) <- varNames
spatTestList[[i]]$sigCount <- 0
#preprocess residList.lon
# residList.lon format:  MSOA11CD  count  week Res_Imp1 Res_Imp2 ...
residListOne <- residList.lon[[i]]
for (k in 1:nVars) {
residListOne[[k]] <- residListOne[[k]] %>% select(-count) %>% rename(GROUP=MSOA11CD) %>% mutate(resid = rowMeans(select(., starts_with("Res_Imp")), na.rm = TRUE)) %>% select(GROUP,week,resid)
}
#preprocess:
locs <- zDat %>% arrange(GROUP,lon,lat)
# NOW IT'S BACK, so remove this...%>% filter(GROUP!="E02000001") #need to deal with City of London --> drop for now
dists <- as.matrix(dist(locs[,c("lon","lat")]))
dists.inv <- 1/dists
diag(dists.inv) <- 0
for (k in 1:nVars) {
residsOne <- residListOne[[k]]
residsSel <- tapply(residsOne$resid,residsOne$GROUP,mean) #take mean
#residsSel <- tapply(residsOne$resid,residsOne$GROUP,"[",1) #take first -- could be a problem, because first is only one WEEK in the interval (and potentially not the same week, though no reason for that)!
spatTestList[[i]][[k]]$moranI <- ape::Moran.I(residsSel,dists.inv)
spatTestList[[i]]$sigCount <- spatTestList[[i]]$sigCount + ifelse(spatTestList[[i]][[k]]$moranI$p.value < 0.05,1,0) #tally sigs
nn1 <- knearneigh1(dist(as.matrix(locs[,c("lat","lon")])))
nn1cor <- cor(residsSel,residsSel[nn1])
spatTestList[[i]][[k]]$nn1cor <- nn1cor
}
spatTestList[[i]]$meanCor <- mean(unlist(lapply(spatTestList[[i]][1:nVars], "[","nn1cor")))
}
spatTestList.lon <- spatTestList
desEff.lon <- matrix(NA,nVars,nIntervals)
for (i in 1:nIntervals) desEff.lon[,i] <- sqrt(1+unlist(lapply(spatTestList.lon[[i]],"[","nn1cor")))[1:nVars]
cols <- c("green","blue","red","cyan","orange","maroon","grey","purple") #have one more color than needed (I hope)
#plot the 3rd from past first!
offsets <- c(0.1*((nIntervals-4):1),NA,-0.1,-0.2,-0.3)
sigEffs.lon <- !CIcont0(estTbl,desEff.lon*seTbl)
pvals.lon <- pvalCoef(estTbl.lon,desEff.lon*seTbl.lon)
gg<-arm::coefplot(estTbl[,nIntervals-3],desEff.lon[,nIntervals-3]*seTbl[,nIntervals-3],varnames=varNames.short,col=cols[nIntervals-3],xlim=c(-0.025,0.027)*ifelse(rescale,10,1),main="Standardized, Adjusted Demographic Effect",plot=T,cex.var=.75)
for (i in (nIntervals:1)[-4]) {
gg<-gg+arm::coefplot(estTbl[,i],desEff.lon[,i]*seTbl[,i],add=T,col=cols[i],offset=offsets[i],varnames=varNames.short)
}
print(gg)
legend(.22,3.7+.3,lty=rep(1,8),lwd=2,col=cols,legend=c("Su20","Fa20","Wi21","Sp21","Su21","Fa21","Wi22","Sp22"),bty='n',cex=.6)
apply(sigEffs.nyc,2,mean)
apply(apply(pvals.nyc,2,p.adjust,method="BH")<.05,2,mean)
apply(sigEffs.lon,2,mean)
apply(apply(pvals.lon,2,p.adjust,method="BH")<.05,2,mean)
#now a graphic
estTbl0 <- estTbl.nyc
rownames(estTbl0) <- c("Ess.","BA+","Older","Young","COPD","Obese","EngOnly","White","Lat/BP","Black","Inc.")
compDat <- reshape2::melt(estTbl0)
names(compDat) <- c("variable.name","Period","NYC.coef")
lonDat <- reshape2::melt(estTbl.lon)
compDat$London.coef <- lonDat$value
sigEff.both <- reshape2::melt(sigEffs.lon*2+sigEffs.nyc+1)
compDat$Significance <- factor(sigEff.both$value,levels=1:4,labels=c("Neither","NYC","London","Both"))
gg <- ggplot(compDat,aes(x=NYC.coef,y=London.coef,label=variable.name,color=Significance,shape=Significance))+geom_point(size=.75,stroke=1.0)+scale_shape_manual(values=c(1,19,3,19)) +
scale_color_manual(values=c("grey10","blue","red","black"))+xlab("Stdz. Adj. Coef.: NYC")+ylab("Stdz. Adj. Coef.: London")+
geom_text_repel(size=2.5,max.overlaps=20,show_guide=FALSE)+geom_vline(xintercept=0,color="darkgrey")+geom_hline(yintercept=0,color="darkgrey")+
geom_smooth(data=within(compDat,Significance<-"Both"),aes(x=NYC.coef,y=London.coef),method="lm",se=T,size=.5,show_guide=FALSE)+ facet_wrap(~Period, ncol=4, nrow=2)+theme(legend.position="bottom")
##+coord_cartesian(xlim=c(-.3,.3),ylim=c(-.3,.3))+
print(gg)
#+theme(legend.position = c(0.875, 0.2))
#+guides(shape=guide_legend(title="Significance"))
par(mfrow=c(1,2))
matplot(desEff.lon,type='p',ylab='Design Effect',lwd=1,cex=.7,ylim=c(1.0,1.33),axes=F,main="London")
matlines(desEff.lon,lwd=1,lty=1)
axis(2);axis(1,labels=varNames,at=1:nVars,pos=1,las=2,cex.axis=.5)
matplot(desEff.nyc,type='p',ylab='Design Effect',lwd=1,cex=.7,ylim=c(1.0,1.33),axes=F,main="NYC")
matlines(desEff.nyc,lwd=1,lty=1)
axis(2);axis(1,labels=varNames,at=1:nVars,pos=1,las=2,cex.axis=.5)
#get time series aggregated
datOne <- as.Date("2020-03-22")
require(readr)
urlfile<-"https://raw.githubusercontent.com/nychealth/coronavirus-data/master/trends/cases-by-day.csv"
ts.agg.nyc <-read_csv(url(urlfile))  #to align NYC/London
ts.agg.nyc.cases <- ts.agg.nyc %>% mutate(date=format(as.Date(date_of_interest , '%m/%d/%Y'), '%Y-%m-%d')) %>% select(date,Cases7Day=CASE_COUNT_7DAY_AVG)
#ts.agg.nyc <- read.csv("../Data/NYC/cases-by-day.csv",header=T)
ts.agg.nyc$date_of_interest <- format(as.Date(ts.agg.nyc$date_of_interest , '%m/%d/%Y'), '%Y-%m-%d')
ts.agg.nyc$day <- as.integer(as.Date(ts.agg.nyc$date_of_interest)-datOne)
ts.agg.nyc <- as_tibble(ts.agg.nyc) %>% rename(date=date_of_interest,total_count=ALL_CASE_COUNT_7DAY_AVG) %>% filter(day>=0 & day%%7==0) %>% mutate(wkNums=1+floor(day/7),total_count=7*total_count) %>% select(date,wkNums,total_count) %>% filter(date<=lastDate) #weekly not daily count; align NYC/London
#scale to 0-1
mx.ts <- max(ts.agg.nyc$total_count)
mn.ts <- min(ts.agg.nyc$total_count)
ts.agg.nyc <- ts.agg.nyc %>% mutate(scaled_count=(total_count-mn.ts)/(mx.ts-mn.ts))
require(readr)
urlfile<-"https://raw.githubusercontent.com/nychealth/coronavirus-data/master/trends/hosp-by-day.csv"
ts.agg.nyc.hosps <-read_csv(url(urlfile))
ts.agg.nyc.hosps <- ts.agg.nyc.hosps %>% mutate(date=format(as.Date(date_of_interest , '%m/%d/%Y'), '%Y-%m-%d')) %>% select(date,Hosps7Day=HOSP_COUNT_7DAY_AVG)
ts.agg.nyc.comps <- ts.agg.nyc.cases %>% left_join(ts.agg.nyc.hosps)
ts.agg.nyc.comps$day <- as.integer(as.Date(ts.agg.nyc.comps$date)-datOne)
ts.agg.nyc.comps <- ts.agg.nyc.comps %>% mutate(week=1+floor(day/7))
ggplot(data=ts.agg.nyc.comps%>% filter(day%%7==0,week>0),aes(y=Hosps7Day/Cases7Day,x=week))+geom_point()+ggtitle("NYC: Relationship between hospitalizations and cases detected, over time")
ts.agg.nyc.compare <- tapply(dat.all$count,dat.all$week,sum,na.rm=T)
ts.agg.nyc.compare[1] <- ts.agg.nyc.compare[1]/8 #correction
#ts.agg.nyc.compare<- ts.agg.nyc.compare[ 1:(length(ts.agg.nyc.compare)-1)]
ts.plot(ts.agg.nyc.compare,main="Compare Adjusted (red) to constructed NYC series",ylab="Weekly Count")
from7day <- 7*(ts.agg.nyc.cases %>% filter(date>="2020-05-18",as.numeric((as.Date(date)-as.Date("2020-05-18"))) %% 7 ==0) %>% select(Cases7Day) %>% unlist())
lines((1:length(from7day)),from7day,col=2)
#get time series aggregated
ts.agg.lon <- dat %>% group_by(week) %>% summarise(total_count=sum(count,na.rm=T)/MI.M)
#adjust first 9 weeks by NYC ratios:
ts.adj <- ts.agg.nyc.comps%>% filter(day%%7==0,week>0,week<10) %>% mutate(adjRatio=Cases7Day/Hosps7Day,week=week+weekOffset) %>% select(week,adjRatio)
ts.agg.lon <- ts.agg.lon %>% left_join(ts.adj)
ts.agg.lon$adjRatio[is.na(ts.agg.lon$adjRatio)] <- 1
ts.agg.lon <- ts.agg.lon %>% mutate(total_count = total_count*adjRatio)
#scale to 0-1
mx.ts <- max(ts.agg.lon$total_count)
mn.ts <- min(ts.agg.lon$total_count)
ts.agg.lon<- ts.agg.lon %>% mutate(wkNums=week-weekOffset, scaled_count=(total_count-mn.ts)/(mx.ts-mn.ts)) %>% select(wkNums,total_count,scaled_count)
ts.dat <- ts.agg.nyc %>% mutate(loc="NYC") %>% select(wkNums,total_count,loc)
ts.dat <- add_row(ts.dat,ts.agg.lon %>% mutate(loc="London") %>% select(wkNums,total_count,loc))
gg_base0 <- ggplot(data=ts.dat,aes(x=wkNums,y=total_count/1000,color=loc))+geom_line(data=ts.dat,stat="smooth",method = "loess", span=.125,formula=y~x)+
scale_color_manual(values=c("blue","red"))+theme(legend.position = "none")+ylab("")+ theme(axis.title.x=element_blank(),axis.text.x=element_blank(), axis.ticks.x=element_blank())+geom_vline(xintercept = unlist(lapply(keyIntervals,"[",1))[-1],col="darkgrey",linetype="dashed")
gg_base <- gg_base0 + scale_y_continuous("",breaks=c(0,20,40,60,80), labels=paste0(c(0,20,40,60,80),"K"))+coord_cartesian(ylim=c(-1,87))
baseR <- F
superImpose<- F
diffWeekNumsBeg <- min(rsltList$NYC$weekNums)-min(rsltList$LON$weekNums)
diffWeekNumsEnd <- max(rsltList$NYC$weekNums)-max(rsltList$LON$weekNums)
# now we can say we have one more var to plot
nVars <- length(rsltList$NYC$betaList)
varNamesNew <- c(varNames,"logPctVaxd")
#use "reorder", based on prefs:
if (is.null(tsOrder)) tsOrder <- 1:nVars
pltDat.all <- NULL
for (k in tsOrder) {
uppBd.lon <- rsltList$LON$betaList[[k]]+1.96*rsltList$LON$seList[[k]]
lowBd.lon <- rsltList$LON$betaList[[k]]-1.96*rsltList$LON$seList[[k]]
uppBd.nyc <- rsltList$NYC$betaList[[k]]+1.96*rsltList$NYC$seList[[k]]
lowBd.nyc <- rsltList$NYC$betaList[[k]]-1.96*rsltList$NYC$seList[[k]]
nWeeks <- max(nWeeks.US,nWeeks.UK)
lonLines <- cbind(lowBd.lon,rsltList$LON$betaList[[k]],uppBd.lon)
wkNums <- rsltList$LON$weekNums
axLabels <- rsltList$LON$dateLabels
if (diffWeekNumsEnd>0) { #more NYC weeks
lonLines <- rbind(lonLines,matrix(NA,diffWeekNumsEnd,dim(lonLines)[2]))
wkNums <- c(wkNums, max(wkNums)+(1:diffWeekNumsEnd))
axLabels <- format.Date(as.Date(as.Date("2020-03-22")+7*(wkNums-1)),"%d%b%y")
}
#correction for plotting -- zero out the early missing period.
rsltList$LON$betaList[["logPctVaxd"]][is.na(rsltList$LON$betaList[["logPctVaxd"]]) & rsltList$LON$weekNums < 70] <- 0 #70 is too large but a safe choice to prevent last values from being changed
rsltList$NYC$betaList[["logPctVaxd"]][is.na(rsltList$NYC$betaList[["logPctVaxd"]])] <- 0
nycLines <- cbind(lowBd.nyc,rsltList$NYC$betaList[[k]],uppBd.nyc)
nycLines<- rbind(matrix(NA,diffWeekNumsBeg,dim(nycLines)[2]),nycLines)
if (diffWeekNumsEnd<0) { #more Lon weeks
nycLines <- rbind(nycLines,matrix(NA,-diffWeekNumsEnd,dim(nycLines)[2]))
}
if (baseR) {
ts.plot(lonLines,col=c("lightblue","blue","lightblue"),xlab="",gpars= list(xaxt="n"),main=varNamesNew[k],xlim=c(min(wkNums),max(wkNums)),ylim=c(min(c(lowBd.lon,lowBd.nyc)),max(c(uppBd.lon,uppBd.nyc))))
legend("topright",lty=c(1,1),col=c("blue","red"),legend=c("London","NYC"),cex=.8,bty='n')
axis(1,at=wkNums,labels=axLabels,las=2,cex.axis=.6);abline(h=0,col=8)
#need to 'pad' nyc lines by the difference in starting points, LON, NYC
matlines(wkNums,nycLines,col=c("pink","red","pink"),lty=c(1,1,1))#when LON & NYC not aligned... use LON as x-axis basis
} else {
ticks <- wkNums[c(T,F,F)] #every 3
pltDatWide <- as.data.frame(cbind(nycLines,lonLines)) %>% rename(beta.nyc=V2,beta.lon=V5) %>% add_column(wkNums,axLabels)
pltDat <- reshape::melt(pltDatWide,id=c("wkNums","axLabels")) %>% filter(variable != "NA",!is.na(value)) %>% mutate(bound=grepl("Bd",variable),city=ifelse(grepl("nyc",variable),"NYC","London"))
#to see London shifted forward to coincide more with NYC:
#pltDat$wkNums[pltDat$city=="London"] <- pltDat$wkNums[pltDat$city=="London"]+4
#Do not use the above, but consider it as an omnibus test (?)
maxHt <- max(pltDat$value)
minHt <- min(pltDat$value)
pltDat <- pltDat %>% left_join(ts.agg.nyc %>% mutate(tot_ct_nyc=scaled_count*(maxHt-minHt)+minHt) %>% select(wkNums,tot_ct_nyc)) %>% left_join(ts.agg.lon %>% mutate(tot_ct_lon=scaled_count*(maxHt-minHt)+minHt) %>% select(wkNums,tot_ct_lon))
gg<-ggplot(data=pltDat,aes(x=wkNums,y=value,group=variable,color=city,linetype=factor(bound)))+geom_line(data=pltDat,stat="smooth",method = "loess", span=.25,formula=y~x)+ stat_smooth(se=F,span=.25,size=.5)+geom_hline(yintercept=0)+scale_color_manual(values=c("blue","red"))+scale_linetype_manual(values=c("solid","dotted"))+ theme(legend.position = "bottom")+guides(linetype=FALSE)+ labs(color="",x="",y="Coefficient")+scale_x_continuous(breaks = ticks,
labels = axLabels[ticks])+theme(axis.text.x=element_text(angle = 90))+labs(title=element_text(varNamesNew[k]))+geom_vline(xintercept=unlist(lapply(keyIntervals,"[",1))[-1],col="darkgrey",linetype="dashed")
if (superImpose) {
gg <- gg+geom_line(data=pltDat,stat="smooth",method = "loess", span=.15, formula=y~x,color="pink",aes(x=wkNums,y=tot_ct_nyc))+annotate(geom="text", x=43+4, y=maxHt+.01, label="NYC Cases", color="red",alpha=.4)
gg_final <- gg+geom_line(data=pltDat,stat="smooth",method = "loess", span=.15, formula=y~x,color="lightblue",aes(x=wkNums,y=tot_ct_lon))+annotate(geom="text", x=42-4, y=maxHt-.01, label="London Cases", color="blue",alpha=.4)
} else {
#grid
gg_final <- grid.arrange( grobs = list(gg_base,gg), heights=c(1.5,7), nrow=2)
}
print(gg_final)
}
pltDat$varname <- varNamesNew[k]
pltDat$varidx <- k
pltDat.all <- rbind(pltDat.all,pltDat)
}
gg <- ggplot(data=pltDat.all%>% filter(varidx %in% tsOrder[1:3]), aes(x=wkNums,y=value,group=variable,color=city,linetype=factor(bound)))+geom_line(stat="smooth",method = "loess", span=.25,formula=y~x)+ stat_smooth(se=F,span=.25,size=.5)+geom_hline(yintercept=0)+scale_color_manual(values=c("red","blue"))+scale_linetype_manual(values=c("solid","dotted"))+ theme(legend.position = "bottom")+guides(linetype=FALSE)+coord_cartesian(ylim=c(-0.35,+0.35))+ labs(color="",x="",y="Coefficient")+scale_x_continuous(breaks = ticks, labels = axLabels[ticks])+ theme(axis.text.x=element_text(angle = 90))+geom_vline(xintercept=unlist(lapply(keyIntervals,"[",1))[-1],col="darkgrey",linetype="dashed")+ facet_grid(facet=varname~.,scales="free")
#alter data
gg_base0$data <- transform(gg_base0$data,label="Incidence")
gg_base <- gg_base0 + scale_y_continuous("",breaks=c(0,25,50,75,100,125,150), labels=paste0(c(0,25,50,75,100,125,150),"K"))+coord_cartesian(ylim=c(-1,155))+facet_grid(facet=label~.)+scale_color_manual(values=c("red","blue"))
grid.arrange( grobs = list(gg_base,gg), heights=c(3,12), nrow=2)
week1nyc <- min(rsltList$NYC$weekNums)
weekBeg <- max(week1nyc,min(rsltList$LON$weekNums)) #limit to overlapping weeks, NYC, London
weekEnd <- min(max(rsltList$NYC$weekNums),max(rsltList$LON$weekNums)) #limit to overlapping weeks, NYC, London
nTicks <- 7
rng <- seq(-.3,.3,length=nTicks)
cols <- rainbow(weekEnd-weekBeg)
oldpar<-par()
par(mfrow=c(2,2),plt=c(.15,.85,.15,.85))
for (i in seq_along(rsltList$NYC$betaList)) {
plot(rng,rng,type='n',xlab="",ylab="",axes=F)
title(main=names(rsltList$NYC$betaList)[i],cex.main=.7,line=0.25)
axis(1,at=rng,label=rep("",nTicks),tck=0.02)
axis(1,cex.axis=.7,lwd=0,line=-1)
axis(2,at=rng,label=rep("",nTicks),tck=0.02)
axis(2,cex.axis=.7,lwd=0,line=-1)
mtext(side=1, line=.65, "NYC.coef",cex=.4)
mtext(side=2, line=.65, "London.coef",cex=.4)
abline(h=0,col=8);abline(v=0,col=8)
sm.NYC <- rsltList$NYC$betaList[[i]]
b <- !is.na(sm.NYC) #nemove NAs for Tukey smoother.
sm.NYC[b] <- smooth(sm.NYC[b])
sm.LON <- rsltList$LON$betaList[[i]]
b <- !is.na(sm.LON) #nemove NAs for Tukey smoother.
sm.LON[b] <- smooth(sm.LON[b])
points(sm.NYC[weekBeg-week1nyc+1],sm.LON[weekBeg],pch=1,col=1,cex=1.5)
arrows(sm.NYC[(weekBeg:(weekEnd-1))-week1nyc+1],sm.LON[weekBeg:(weekEnd-1)],sm.NYC[((weekBeg+1):weekEnd)-week1nyc+1],sm.LON[(weekBeg+1):weekEnd],length=0.025,col=cols)
points(sm.NYC[weekEnd-week1nyc+1],sm.LON[weekEnd],pch=4,col=1,cex=1.5)
if (i%%4==0) print("<P style='page-break-before: always'>")
}
par(oldpar)
knitr::include_graphics('../Results/NYC/dailyByBoroAge.png')
#SCRATCH - TESTS -
#ggplot(data=pltDat,aes(x=wkNums,y=value,group=city,color=city,linetype=bound))+stat_smooth(data=pltDat,se=F,span=.25,size=.5,aes(alpha=1-1*bound/2))+geom_hline(yintercept =0)+scale_color_manual(values=c("pink","red","pink","lightblue","blue","lightblue"))+scale_linetype_manual(values=c(1,1))
#ggplot(data=pltDat,aes(x=wkNums,y=value,group=variable,color=city,linetype=bound))+stat_smooth(data=pltDat,se=F,span=.25,size=.5,aes(alpha=1-1*bound/2))+geom_hline(yintercept =0)+scale_color_manual(values=c("pink","red","pink","lightblue","blue","lightblue"))+scale_linetype_manual(values=c(1,1))
#+scale_x_continuous(breaks=1:65,labels=axLabels)
#datage <- as_tibble(read_stata("../Data/London/age.dta"))
datage <- as_tibble(read_stata("../Data/London/agerec.dta")) %>% select(-age2) %>% rename(age2=age3) %>% mutate(age=as.character(haven::as_factor(age2)))
datage <- datage %>% mutate(borocode=case_when(borcode %in% c("07","20","19","22","28","33")~1, borcode %in% c("02","04","11","12","16","23","25","26","30","31")~2, borcode %in% c("03","10","14")~3, borcode %in% c("06","08","21","24","29","32")~4, borcode %in% c("05", "09", "13","15","17","18","27")~5))
datage <- datage %>% mutate(LondSR=case_when(borocode %in% c(1)~"Central London",borocode %in% c(2)~"East London",borocode %in% c(3)~"North London",borocode %in% c(4)~"South London", borocode %in% c(5)~"West London")) %>% filter(!is.na(borocode))
dateOffset <- as.Date("2020-05-18") - as.Date(min(datage$date))
datage$date3 <- datage$date2 - min(datage$date2) - dateOffset + 1
ggplot(datage, aes(date3,y=cases*(100000/pop),group=age2,color=age)) + stat_smooth(se=FALSE, method="loess", span=.3,size=.5) + theme(legend.position="bottom")+ ylab(expression(Delta*" per 100K"))+facet_wrap(.~LondSR,nrow=2,ncol=3)+labs(title="7-day New Cases of Covid-19 in London by Age Group", caption = "Data source: ONS")+coord_cartesian(ylim=c(0, 112.5))+xlim(1,max(datage$date3))+xlab(paste0("Days since 18May20"))
#ggplot(datage, aes(week,y=cases*(100000/pop),group=age2,color=age)) + stat_smooth(se=FALSE, span=.5,size=.5) + theme(legend.position="bottom")+ ylab(expression(Delta*" per 100K"))+xlim(45,70)+facet_wrap(.~borocode,nrow=2,ncol=3)+facet_wrap(.~LondSR,nrow=2,ncol=3)+labs(title="7-day New Cases of Covid-19 in London by Age Group, Third wave", caption = "Data source: ONS")+coord_cartesian(ylim=c(0, 200))
lastWeek <- max(dat.all$week)
arrDat <- dat.all %>% select(GROUP,borocode,BOROUGH_GROUP,week,count,base,BAplus,Essential,MedianInc,PercentWhite) %>% mutate(rate=100000*count/base) %>% arrange(borocode,GROUP,week)
arrDat$lastrate <- rep((arrDat %>% filter(week==lastWeek-0))$rate,each=lastWeek) #week 42 good
arrDat <- arrDat %>% arrange(borocode,week,lastrate) %>% group_by(borocode,week) %>% mutate(rank=rank(PercentWhite),n=n(),frac=(rank-1)/(n-1)) #or rank(lastRate)
pg <- ggplot(data=arrDat%>% mutate(rate=pmax(0,pmin(500,rate))) %>% filter(week>=28,week<=lastWeek), aes(week, frac)) +
facet_wrap(.~BOROUGH_GROUP,nrow=2,ncol=3) + geom_raster(aes(fill = rate)) + scale_fill_gradient2(low="white", mid="yellow", high="red", guide="colorbar", na.value="white")+theme(legend.position = c(0.85, 0.2)) #or rate=log(rate)
print(pg)
#lattice::levelplot((rate)~week*frac|BOROUGH_GROUP,col.regions=rev(heat.colors(100)),data=arrDat%>% mutate(rate=pmin(500,rate)) %>% filter(week>=28,week<=lastWeek))
dat.london <- dat %>%
select(week, date, MSOA11CD, Localauthority, pop18, borocode,
cases, count, Essential, MedianInc, PercentWhite) %>%
mutate(rate=100000*count/pop18, week=week-weekOffset) %>%
group_by(borocode,MSOA11CD,week) %>% summarize(rate=mean(rate,na.rm=T), Localauthority=Localauthority,Essential=Essential, MedianInc=MedianInc, PercentWhite=PercentWhite) %>% mutate(LondSR=case_when(borocode %in% c(1)~"Central London",borocode %in% c(2)~"East London",borocode %in% c(3)~"North London",borocode %in% c(4)~"South London", borocode %in% c(5)~"West London"))
lastWeek <- max(dat.london$week)
dat.london$lastrate <- rep((dat.london %>% filter(week==lastWeek))$rate,each=lastWeek)
arrDat <- dat.london %>% arrange(borocode,week,lastrate) %>% group_by(borocode,week) %>% mutate(rank=rank(lastrate),n=n(),frac=(rank-1)/(n-1)) #or rank(lastRate)
pg <- ggplot(data=arrDat%>% mutate(rate=pmax(0,pmin(500,rate))) %>% filter(week>=28,week<=lastWeek), aes(week, frac)) +
facet_wrap(.~LondSR,nrow=2,ncol=3) + geom_raster(aes(fill = rate)) + scale_fill_gradient2(low="white", mid="yellow", high="red", guide="colorbar", na.value="white")+theme(legend.position = c(0.85, 0.2)) #or rate=log(rate)
print(pg)
