---
title: "Tale of Two Cities: London and New York City during Covid-19"
header-includes:
   - \usepackage{todonotes}
   - \usepackage{hyperref}
output: 
  bookdown::pdf_document2:
    number_sections: true
    toc: false
date: "Compiled on `r format(Sys.time(), '%a %b %d %H:%M %Z')`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#library(flexdashboard)
library(reshape2)
library(dplyr) 
library(ggplot2)
library(ggrepel)
library(GGally)
require(ggExtra)
require(grid)
require(gridExtra)
library(lubridate)
require(RCurl)
require(rvest)
require(haven)
require(tibble)
require(foreach)
require(parallel)
require(doParallel)
require(ape) #for Moran's I
require(vtable)

evalFlag <- T #use eval option to turn off chunks
DRAFT <- F 
```

# Introduction

This file contains code to reconstruct all figures used in the paper. Tables (for appendix) were created from LaTeX files generated within this code as well.

```{r common-code-1, echo=F, message=FALSE, warning=FALSE, include=T}
rsltList <- NULL #base for betas/se, both NYC and London
#note: currently does not read in resids!
#CHANGE THIS (to F) to re-do newly imputed runs:
readFromFile <- c("NYC","LON","BOTH","NEITHER")[3]
logVarNames <- c("PercentHispanic","PercentBlack","MedianInc") #take log of these variables before using them.
#redoVarNames <- logVarNames #to save time on London re-runs when taking logs.
redoVarNames <- c()
tsOrder <- c(1,11,8,2,3:7,9,10,12) #for time series plots
removeSI <- F #important choice for NYC time series
splitBoros <- T # divide Boros into multiple regions for variation on FE model
stdModel <- c(F,F) #nycHH/LAs as REs in 7 key intervals (NYC,Lon)
mimFactor <- ifelse(stdModel[2],1,1) #divisor - for debugging with smaller number of Multiple Imputes
```

# Figures:

```{r helper-fn-genl,  include=F, echo=F, warning=F,message=F}
takeLog <- function(data,selVars) {
  if (is.null(selVars)) return(data) #nothing to do
  vNames <- colnames(data)
  idx <- match(selVars,vNames)
  if (any(is.na(idx))) stop(paste0("Bad list of variables to log: ",selVars,"\n"))
  for (i in idx) {
    logZeroFix <- ifelse(data[,vNames[i]]==0,0.1,0) #swap log(0)
    data[,vNames[i]] <- log(data[,vNames[i]]+logZeroFix)
  } 
  return(data)
}
```

```{r load-data-NYC-1, include=F, echo=F, warning=F,message=F}
#Can we get %obesity, %white, %english spoken 
dat <- as_tibble(read.csv(paste0("../Data/NYC/daily_cases_boro_Zip.csv"))) %>% rename(GROUP=MODIFIED_ZCTA)
supp <- readr::read_csv("../Data/NYC/NewData10220.csv") 
supp.health <- readr::read_csv("../Data/NYC/Health.data.csv") 
supp.poli <- readr::read_csv("../Data/NYC/Polidata.csv") %>% select(GROUP,Repub=Voting.Rep.2019.scarbough) #not using this right now.
#nbhd approx from:
#approx from: https://www1.nyc.gov/assets/doh/downloads/pdf/epi/nyc_comhealth_atlas10.pdf
nyc.nbhd <-readr::read_csv("../Data/NYC/borocodes_marc.csv") %>% mutate(nbhdcode=as.numeric(factor(GroupName))) %>% select(ZIP,Group,GroupName,nbhdcode) %>% rename(GROUP=ZIP,nbhdName=GroupName)
eng <- readr::read_csv('../Data/NYC/English.csv')

dat.orig <- dat %>% left_join(supp%>%rename(GROUP=Zip)) %>% left_join(supp.health%>%mutate(Asthma=as.numeric(Asthma),COPD=as.numeric(COPD),Obese = as.numeric(Obese)) %>% select(GROUP,Asthma,COPD, Obese)) %>% left_join(nyc.nbhd) %>% arrange(GROUP,day) %>% filter(GROUP!=11096 & BOROUGH_GROUP != 'Staten Is') # has more variables
supp <- supp %>% select(GROUP=Zip, PercentPoverty2017, Retail, Wholesale, `Transportation and Warehousing`, `Entertainment, Accommodation and Food Services, etc`, `Employment, Health Care and Social Assistance`, `Employment, Accomodation and Food Services`, `Production, Transportation, and Material Moving`,Service)
supp$Essential <- apply(supp[,-c(1:2,10)],1,sum,na.rm=T) #collapse to build essential worker
dat.orig <- dat.orig %>% left_join(supp)
dat.orig <- dat.orig %>% left_join(eng)
dat.orig <- dat.orig %>% rename(count=COVID_CASE_COUNT,base=POP_DENOMINATOR)
dat.orig <- dat.orig %>% mutate(MedianInc = `Median Income`/1000) #scale should be in 1000s
demog.NYC <- dat.orig %>% filter(day==1) %>% mutate(BAplus=Bachelor+GraduateORprofessional,PercentOlder=PercentPop65to74+PercentPop75plus,PercentYounger=PercentPop0to17) %>% select(GROUP,BOROUGH_GROUP, Essential,BAplus,PercentOlder,PercentYounger, COPD,Obese,OnlyEngAtHome.acs,PercentWhite,PercentHispanic,PercentBlack,MedianInc)

#log as requested
demog.NYC <- takeLog(demog.NYC,logVarNames)  

names(demog.NYC) <-  gsub(".acs","",names(demog.NYC))
st(demog.NYC,out="latex",file="demog.NYC.table.tex") #produce LaTeX for table


ggpairs(data=demog.NYC,columns=3:13,aes(colour=BOROUGH_GROUP),lower = list(continuous = wrap(ggally_points, size = .3)),upper=list(continuous=wrap(ggally_cor,cex=1.4)),diag = list(continuous = wrap("densityDiag", alpha = 0.5)),title = "Correlation matrix: NYC")+theme_grey(base_size = 5)
```

  
```{r time-series-coef-1,  include=DRAFT, echo=F, warning=F,message=F}
  rescale <- T
  bnames <- sort(unique(dat.orig$BOROUGH_GROUP))
  dat.orig$borocode <- pmin(5,match(dat.orig$BOROUGH_GROUP,bnames)) #handle staten island, which goes by two names in the data
  ## Aligns the "date" with the day in NYC DATA
  ## TODO
  nWeeks.US <- nWeeks <- floor(max(dat.orig$day)/7)+1 #there's an extra week because "week 1" is based on May18 alone
  varNames <- varNames.US<-  c("Essential","BAplus","PercentOlder","PercentYounger","COPD", "Obese", "OnlyEngAtHome.acs" ,"PercentWhite" , "PercentHispanic", "PercentBlack" ,"MedianInc") #names of key variables for regression
  varNames.short <- c("Essential","BAplus","Older","Younger","COPD", "Obese", "English1st" ,"White" , "Hispanic", "Black" ,"Income")
  
#log as requested
  dat.orig <- takeLog(dat.orig,logVarNames)  

  nVars <- length(varNames)
  #initialize lists to store regression runs
  seList <- betaList <- vector("list",nVars)
  betaList[1:nVars]<-rep(NA,nWeeks)
  seList[1:nVars]<-rep(NA,nWeeks)
  names(betaList) <- names(seList) <- varNames
  dat.all <- NULL # crude way to build a dataset
  for (i in 1:nWeeks) { 
    endDay <- 1+(i-1)*7
    dat.last <- dat.orig%>%filter(day>=endDay-7, day <= endDay) 
    if (i>1) {
      ## don't take max - min; take last()- first()
      ## let it go negative and then set negatives to NAs
      dat.weekly <- dat.last %>% group_by(BOROUGH_GROUP, GROUP) %>% summarize(count = last(count), .by_group=TRUE) 
      dat.temp <- dat.weekly
      dat.weekly <- dat.weekly %>% left_join(dat.hold %>% select(BOROUGH_GROUP, GROUP,firstCount))
      dat.weekly <- dat.weekly %>% mutate(count = count - firstCount)  
      dat.weekly$count[dat.weekly$count < 0] <- NA
      dat.hold <- dat.temp %>% rename(firstCount=count)
    } else {
      dat.weekly <- dat.last %>% filter(day==1) %>% mutate(count=count/8) #one day only! May18 - approx. 8 weeks into the pandemic
      dat.hold <- dat.weekly %>% mutate(firstCount=count*8) #restore
    }
    if (i==1) {
       dat.last <- dat.orig%>%filter(day==max(dat.orig$day))%>%select(BOROUGH_GROUP,GROUP) %>% right_join(dat.weekly,by=c("BOROUGH_GROUP","GROUP"))
    } else dat.last <- dat.orig%>%filter(day==max(dat.orig$day))%>%select(-count) %>% right_join(dat.weekly,by=c("BOROUGH_GROUP","GROUP"))  #this line was improved ; before all one case, and varying 'by group'
    if(removeSI) dat.last <- dat.last %>% filter(borocode!=5)
    dat.last$PercentOlder <- dat.last$PercentPop65to74+dat.last$PercentPop75plus  
    dat.last$PercentYounger <- dat.last$PercentPop0to17
    dat.last$BAplus <- dat.last$Bachelor+dat.last$GraduateORprofessional 
    if (i==1) {
      dat.all <- dat.last %>% mutate(week=i,BA1=BAplus,BA2=0,BA3=0)
    } else dat.all <- rbind(dat.all,dat.last %>% select(-.by_group,-firstCount) %>% mutate(week=i,BA1=BAplus*(week<20),BA2=BAplus*(week>=20)*(week<35),BA3=BAplus*(week>=35))) 
    for (k in 1:nVars) {
      fmla <- as.formula(paste0("count~",varNames[k],"+factor(borocode)+offset(log(base))")) 
      fit<-glm(formula=fmla,data=dat.last,family="quasipoisson")
      seList[[k]][i] <- summary(fit)$coef[varNames[k],"Std. Error"]
      betaList[[k]][i] <- fit$coefficients[varNames[k]]
    }
  }   
  dateLabels <- format.Date(as.Date(as.Date("2020-05-18")+7*(0:(nWeeks-1))),"%d%b") 
   #format.Date(as.Date(as.Date("2020-05-18")+7*(0:nWeeks)),"%d%b") 
  if (rescale) {
    for (i in 1:nWeeks) {
      #PROBLEM LINE:
      dat.last <- dat.all %>% filter(week==i)
      sds <- sapply(dat.last[,varNames],sd,na.rm=T)
      for (k in 1:nVars) {
        seList[[k]][i] <- seList[[k]][i]*sds[k]
        betaList[[k]][i] <- betaList[[k]][i]*sds[k]
      }
    }
  }
  rsltList$NYC <- list(betaList=betaList,seList=seList,dateLabels=dateLabels,weekNums=8+1:nWeeks) #hard-coded the offset for alignment with London
  #
  # robustness check
  # ggplot(dat.last,aes(x=BAplus,y=100000*count/base,group=factor(borocode),col=factor(borocode)))+geom_point()+geom_smooth(method="loess")
```

```{r helper-fn-pois,  include=F, echo=F, warning=F,message=F}
glm.disp <- function(x) {
  dev.res <- residuals(x, type = "deviance")
  disp.est <- sum(dev.res^2) #this is chi-sq w/ df from the model
  df <- df.residual(x) #use built-in function
  list(dispersion = disp.est/df, p = 1 - pchisq(disp.est, df))
}

 fitMImepois <- function(fmla,data,varName,M,...) {
    beta <- rep(NA,M)
    se2 <- rep(NA,M)
    resids <- vector("list",M)
    for (m in 1:M) {
      dat2 <- data[data$imputeNumber==m,]
      fit <-lme4::glmer(formula=fmla,data=dat2,family="poisson",...)
      dispInflator <- glm.disp(fit)$dispersion #var not s.d. here
      beta[m]<- summary(fit)$coef[varName,"Estimate"]
      se2[m] <- dispInflator*summary(fit)$coef[varName,"Std. Error"]^2
      resids[[m]] <- residuals(fit, type = "pearson") 
    }
    nonMsg <- !is.na(se2) #model failure results in NaN for se
    betaMn <- mean(beta[nonMsg])
    B <- var(beta[nonMsg]) #does the n-1 denom
    V <- mean(se2[nonMsg]) + B*(1+1/M)
    list(beta=betaMn,se=sqrt(V),nonMsg=nonMsg,resids=resids)
 }
 
 fitMIqpois <- function(fmla,data,varName,M) {
    beta <- rep(NA,M)
    se2 <- rep(NA,M)
    for (m in 1:M) {
      dat2 <- data[data$imputeNumber==m,]
      fit <-glm(formula=fmla,data=dat2,family="quasipoisson")
      beta[m]<- coef(fit)[varName]
      se2[m] <- summary(fit)$coef[varName,"Std. Error"]^2
    }
    nonMsg <- !is.na(se2) #model failure results in NaN for se
    betaMn <- mean(beta[nonMsg])
    B <- var(beta[nonMsg]) #does the n-1 denom
    V <- mean(se2[nonMsg]) + B*(1+1/M)
    list(beta=betaMn,se=sqrt(V),nonMsg=nonMsg)
  }

```

```{r mlm-1, include=T, echo=F, warning=F,message=F}
#need to know when the last 'adjusted' date is...
keyIntervals <- list(summer20=c(9,24),fall20=c(25,35),winter21=c(36,48),spring21=c(49,59),summer21=c(60,69),fall21=c(70,80),winter22=c(81,Inf),spring22=c(100,Inf)) #there is a smaller fall21 group (last two cover rise in delta var.) might shift latesummer to start 1 or 2 weeks later
nIntervals <- length(keyIntervals)
mlm.fit <- mlm.full <- residList.nyc <- vector("list",nIntervals)
names(mlm.fit) <- names(mlm.full) <- names(residList.nyc ) <- names(keyIntervals)
estTbl <- matrix(NA,nVars,nIntervals)
seTbl <-seTbl.full <- matrix(NA,nVars,nIntervals)
dimnames(seTbl) <- dimnames(estTbl) <- dimnames(seTbl.full) <- list(varNames,names(keyIntervals))
keyIntervals.nyc <- lapply(keyIntervals,"-",8) #NYC starts May18
fnPrefix <- paste0("estNYC",ifelse(stdModel[1],"","_NYHHfe"))
#allow split queens:
dat.all$nbhdNew <- dat.all$nbhdcode #default
if (splitBoros) {
  dat.all$nbhdNew[dat.all$nbhdcode %in% c(4,20,25,32)] <- 101 #Bx E/W
  dat.all$nbhdNew[dat.all$nbhdcode %in% c(2,5,16,17)] <- 102 #Bx middle
  dat.all$nbhdNew[dat.all$nbhdcode %in% c(7,10,18)] <- 301 #Mn No
  dat.all$nbhdNew[dat.all$nbhdcode %in% c(13,39,40)] <- 302 #Mn middle
  dat.all$nbhdNew[dat.all$nbhdcode %in% c(9,15,21,22)] <- 303 #Mn So
  dat.all$nbhdNew[dat.all$nbhdcode %in% c(3,14,27)] <- 201 #Bk No
  dat.all$nbhdNew[dat.all$nbhdcode %in% c(4,6,11,12)] <- 202 #Bk Ea
  dat.all$nbhdNew[dat.all$nbhdcode %in% c(1,12,34,35,38)] <- 203 #Bk So
  dat.all$nbhdNew[dat.all$nbhdcode %in% c(28,33,41,42)] <- 401 #Qn W
  dat.all$nbhdNew[dat.all$nbhdcode %in% c(19,30,36)] <- 402 #Qn So
  dat.all$nbhdNew[dat.all$nbhdcode %in% c(8,24,26,33)] <- 403 #Qn N/E
  dat.all$nbhdNew[dat.all$nbhdcode %in% c(23,29,31,37)] <- 501 #SI all
}
if (readFromFile %in% c("NYC","BOTH")) {
  load(file=paste0("../Data/NYC/",fnPrefix,".Rdata"))
} else {  
  numCores <- parallel::detectCores()
  cl <- parallel::makeCluster(numCores, setup_strategy = "sequential") 
  registerDoParallel(cl)
  for (i in 1:nIntervals) {
    mlm.fit[[i]] <- residList.nyc[[i]] <- vector("list",nVars)
    names(mlm.fit[[i]]) <- names(residList.nyc[[i]]) <- varNames
    subDat <- dat.all[dat.all$week>=keyIntervals.nyc[[i]][1] & dat.all$week<=keyIntervals.nyc[[i]][2],]
    subDat$count <- round(subDat$count) #matters only for week 1.
    residList.nyc[[i]]$designMat <- subDat %>% filter(!is.na(count)) %>% select(GROUP,week) #specific to each key interval
    commonSfx <- "+poly(week,3)+offset(log(base))+(1|GROUP)"
    mlm.fit[[i]] <- foreach (k=1:nVars) %dopar% lme4::glmer(formula=as.formula(paste0("count~",varNames[k],ifelse(stdModel[1], "+factor(borocode)","+factor(nbhdNew)"),commonSfx)), data=subDat, family="poisson")
    for (k in 1:nVars) { #clean up 
      residList.nyc[[i]][[k]] <- residuals(mlm.fit[[i]][[k]],type="pearson")
      dispInflator <- sqrt(glm.disp(mlm.fit[[i]][[k]])$dispersion)
      seTbl[k,i] <- summary(mlm.fit[[i]][[k]])$coef[varNames[k],"Std. Error"]*dispInflator #to handle overdispersion
      estTbl[k,i] <- summary(mlm.fit[[i]][[k]])$coef[varNames[k],"Estimate"]
    }
  }
  stopCluster(cl)
  save(seTbl,estTbl,mlm.fit,residList.nyc,file=paste0("../Data/NYC/",fnPrefix,".Rdata"))
}   
if (rescale) {
  for (i in 1:nIntervals) {
      subDat <- dat.all[dat.all$week>=keyIntervals.nyc[[i]][1] & dat.all$week<=keyIntervals.nyc[[i]][2],]
      sds <- sapply(subDat[,varNames],sd,na.rm=T)
      seTbl[,i] <- seTbl[,i]*sds
      estTbl[,i] <- estTbl[,i]*sds
  }
}
#remove after rerunning NYC models
#colnames(estTbl)[colnames(estTbl)=="earlysummer21"] <- "summer21"
#colnames(seTbl)[colnames(seTbl)=="earlysummer21"] <- "summer21"
colnames(estTbl)[colnames(estTbl)=="spring23"] <- "spring22"
colnames(seTbl)[colnames(seTbl)=="spring23"] <- "spring22"
estTbl.nyc <- estTbl
seTbl.nyc <- seTbl
```

```{r spat-func, include=F, echo=F, warning=F,message=F}
  knearneigh1 <- function(distmat) {
    mat <- as.matrix(distmat) # convert from distmat (compressed) to matrix (redundant)
    diag(mat) <- Inf # to avoid this distance being picked
    nn <- apply(mat,1,which.min)
    nn
  }
```

```{r processResids-nyc, include=F, echo=F, warning=F,message=F}
load("../Data/NYC/nycZcta5centroid.Rdata") # ZIP Centroids  'zDat'
spatTestList <- vector("list",nIntervals)
names(spatTestList) <- names(keyIntervals)
for (i in 1:nIntervals) {
  spatTestList[[i]] <- vector("list",nVars)
  names(spatTestList[[i]]) <- varNames
  spatTestList[[i]]$sigCount <- 0
  #preprocess resids:
  residListOne <- residList.nyc[[i]]
  designOne <- residListOne$designMat
  startWeek <- min(designOne$week) #choose one week
  locs <- left_join(designOne %>% arrange(GROUP,week) %>% group_by(GROUP) %>% filter(row_number()==1),zDat %>% rename(lat=intptlat,lon=intptlong,GROUP=zcta5) %>% mutate(GROUP=as.double(GROUP)))
  dists <- as.matrix(dist(locs[,c("lon","lat")]))
  dists.inv <- 1/dists
  diag(dists.inv) <- 0
  for (k in 1:nVars) {
    residsOne <- cbind(residListOne[[k]],designOne) # to get weeks
    colnames(residsOne)[1] <- "resid"
    residsSel <- tapply(residsOne$resid,residsOne$GROUP,mean) #take mean
    #residsSel <- tapply(residsOne$resid,residsOne$GROUP,"[",1) #take first
    spatTestList[[i]][[k]]$moranI <- ape::Moran.I(residsSel,dists.inv)
    spatTestList[[i]]$sigCount <- spatTestList[[i]]$sigCount + ifelse(spatTestList[[i]][[k]]$moranI$p.value < 0.05,1,0) #tally sigs
    nn1 <- knearneigh1(dist(as.matrix(locs[,c("lat","lon")])))
    nn1cor <- cor(residsSel,residsSel[nn1])
    spatTestList[[i]][[k]]$nn1cor <- nn1cor
  }
  spatTestList[[i]]$meanCor <- mean(unlist(lapply(spatTestList[[i]][1:nVars], "[","nn1cor")))
}  
spatTestList.nyc <- spatTestList
desEff.nyc <- matrix(NA,nVars,nIntervals)
for (i in 1:nIntervals) desEff.nyc[,i] <- sqrt(1+unlist(lapply(spatTestList.nyc[[i]],"[","nn1cor")))[1:nVars]
```

```{r coefplt-fns, include=T, echo=F, warning=F,message=F}
CIcont0 <- function(est,se) {
  ifelse(est-1.96*se>0|est+1.96*se<0,F,T)
}
pvalCoef <- function(est,se) {
  zAbs <- abs(est/se)
  pvals <- 2-2*pnorm(zAbs)
  pvals
}
```

```{r coefplt-1, results=F,include=T, echo=F, fig.cap="Adjusted Regression Coefficients: NYC", warning=F,message=F,fig.dim=c(6,8)}
cols <- c("green","blue","red","cyan","orange","maroon","grey","purple") #have one more color than needed (I hope)
offsets <- c(0.1*((nIntervals-4):1),NA,-0.1,-0.2,-0.3)
sigEffs.nyc <- !CIcont0(estTbl,desEff.nyc*seTbl)
pvals.nyc <- pvalCoef(estTbl.nyc,desEff.nyc*seTbl.nyc)
#trick is to plot the 3 from last first and use negative offsets on last 3
gg<-arm::coefplot(estTbl[,nIntervals-3],desEff.nyc[,nIntervals-3]*seTbl[,nIntervals-3],varnames=varNames.short,col=cols[nIntervals-3],xlim=c(-0.025,0.025)*ifelse(rescale,10,1),offset=-0.1,plot=T,cex.var=.75) + theme(axis.text = element_text(size = (2)))
   #i don't think offset does anything here
for (i in (nIntervals:1)[-4]) {
  gg<-gg+arm::coefplot(estTbl[,i],desEff.nyc[,i]*seTbl[,i],add=T,col=cols[i],offset=offsets[i],varnames=varNames.short)
}
print(gg)
```


```{r vax-nyc, include=DRAFT, echo=F, warning=F,message=F}
load("../Data/NYC/vaxDat.Rdata") #allVaxDat

mxDt <- max(allVaxDat$DATE)
mnDt <- min(allVaxDat$DATE)
lastVaxDat <- as_tibble(allVaxDat) %>% filter(DATE==mxDt) %>% rename(GROUP=MODZCTA,maxpct=PERC_1PLUS) %>% select(GROUP,maxpct)
firstVaxDat <- as_tibble(allVaxDat) %>% filter(DATE==mnDt) %>% rename(GROUP=MODZCTA,minpct=PERC_1PLUS) %>% select(GROUP,minpct)
diffVaxDat <- left_join(firstVaxDat,lastVaxDat) %>% mutate(PERC_1PLUS=maxpct-minpct)

#use firstVaxDat to interpolate to 2021-01-14, when it was effectively 0.
daysBetween <- as.integer(as.Date(mnDt)-as.Date("2021-01-14"))
interpol <- function(x,days) {
  #interpolate a quadratic with x as endpt after days days.
  dayRange <- (0:days)/days # keep in [0,1]
  mat <- outer(x,dayRange^2,"*")
  colnames(mat) <- 0:days
  rownames(mat) <- names(x)
  return(mat)
}
endLevelWZIP <- firstVaxDat$minpct
names(endLevelWZIP) <- firstVaxDat$GROUP
wideMat <- interpol(endLevelWZIP,daysBetween)
newVals <- reshape2::melt(wideMat)
colnames(newVals) <- c("GROUP","Day","pct")
newVals$DATE <- as.Date(mnDt)-newVals$Day
newVals$Day <- NULL
#append allVaxDat
newVals <- bind_rows(newVals,(allVaxDat%>%select(GROUP=MODZCTA,pct=PERC_1PLUS,DATE) %>% mutate(DATE=as.Date(DATE))))
#subset and generate week-based:
weeklyVaxDat <- newVals%>%mutate(daysFromStart =as.integer(as.Date(DATE)-as.Date("2020-05-18"))) %>% filter(daysFromStart%%7<=1) %>% mutate(week=1+floor(daysFromStart/7)) %>% as_tibble()  %>% arrange(GROUP,week,daysFromStart) %>% group_by(GROUP,week) %>% filter(row_number()==1)#allow for a missed vax report date.

#choose the interval:
keyInt <- 4
subDat <- dat.all[dat.all$week>=keyIntervals.nyc[[keyInt]][1] & dat.all$week<=keyIntervals.nyc[[keyInt]][2],]
#choose which one
subDat <- subDat %>% left_join(lastVaxDat) %>% rename(PERC_1PLUS=maxpct)
#subDat <- subDat %>% left_join(firstVaxDat) %>% rename(PERC_1PLUS=minpct)
#subDat <- subDat %>% left_join(diffVaxDat)

#this code is broken - it redoes the last interval with vax as a control. not sure why...
#redo fits, with addl control
##
if (F) {
#
mlm.fit2 <- vector("list",nVars)
i <- nIntervals
for (k in 1:nVars) {
      fmla <- as.formula(paste0("count~",varNames[k],"+log(PERC_1PLUS)+factor(borocode)+poly(week,3)+offset(log(base))+(1|GROUP)"))
      mlm.fit2[[k]] <- fit <- lme4::glmer(formula=fmla,data=subDat, family="poisson")
      dispInflator <- sqrt(glm.disp(fit)$dispersion)
      seTbl[k,i] <- summary(fit)$coef[varNames[k],"Std. Error"]*dispInflator #to handle overdispersion
      estTbl[k,i] <- summary(fit)$coef[varNames[k],"Estimate"]
}

if (rescale) {
  sds <- sapply(subDat[,varNames],sd,na.rm=T)
  seTbl[,i] <- seTbl[,i]*sds
  estTbl[,i] <- estTbl[,i]*sds
}

#update
estTbl.nyc <- estTbl
seTbl.nyc <- seTbl
}
###


dat.sub <- left_join(weeklyVaxDat,dat.all)
minWeek <- min(dat.sub$week)
maxWeek <- max(dat.sub$week)
fit <- vector('list',1+maxWeek-minWeek)
se <- beta <- vector("numeric",1+maxWeek-minWeek)
for (i in minWeek:maxWeek) {
  fit[[i-minWeek+1]] <- fit0 <-  glm(formula=count~I(log(pct))+factor(borocode)+offset(log(base)),data=dat.sub %>% filter(week==i),family="quasipoisson")
  sd.local <- sd(log(dat.sub$pct[dat.sub$week==i])) #for stdzn
  se[i-minWeek+1] <- as.numeric(summary(fit0)$coef["I(log(pct))","Std. Error"])*sd.local
  beta[i-minWeek+1] <- fit0$coefficients["I(log(pct))"]*sd.local
}

#append these to existing se, beta for NYC:
rsltList$NYC$seList$logPctVaxd <- rep(NA,length(seList[[1]])) #placeholder
rsltList$NYC$betaList$logPctVaxd <- rep(NA,length(seList[[1]])) #placeholder
rsltList$NYC$seList$logPctVaxd[minWeek:maxWeek] <- se #note these are stdzd
rsltList$NYC$betaList$logPctVaxd[minWeek:maxWeek] <- beta #note these are stdzd

#one viz:
pltDatW <- data.frame(week=minWeek:maxWeek,beta,se)
pltDatW <- pltDatW %>% mutate(lowBd=beta-1.96*se,uppBd=beta+1.96*se) 

pltDatL <- reshape::melt(pltDatW,id=c("week")) %>% filter(variable != "se") %>% mutate(bound=grepl("Bd",variable))

gg<-ggplot(data=pltDatL,aes(x=week,y=value,group=variable,linetype=factor(bound)))+geom_line(data=pltDatL,stat="smooth",method = "loess", span=.25,formula=y~x)+ stat_smooth(se=F,span=.25,size=.5)+geom_hline(yintercept=0)+scale_linetype_manual(values=c("solid","dotted"))+ theme(legend.position = "none")+ylab("Standardized Coefficient")+ggtitle("Borough-adjusted effect of (logged) fully vaccinated rate on COVID incidence rate")
#save viz. for full set of plots
#print(gg)

#attempt to viz the curves:
myDat <-subDat %>% group_by(GROUP) %>%summarise(rawrate=sum(count),PercentVaxDose=log(mean(PERC_1PLUS)),borocode=min(borocode),base=min(base)) %>% ungroup %>% mutate(CovidRate=100000*rawrate/base)
gg <- ggplot(myDat, aes(x=PercentVaxDose,y=CovidRate,group=borocode,col=factor(borocode,labels=bnames[ifelse(removeSI,-5,-6)])))+geom_point()+geom_smooth(span=1)+theme(legend.title=element_blank())+ylab("Covid Rate/100000")+xlab("Avg % Fully Vaxxed (logged)")+ggtitle("Feb-Apr 2021 (3 months)")
print(gg)
```



```{r gam-1, include=F, echo=F, warning=F,message=F}
GAM.RUN<-F
#special run
if (GAM.RUN) {
  require(gam); require(mgcv)
  fit<-gam(formula=count~s(Essential)+s(week)+I(factor(nbhdcode))+offset(log(base)),data=dat.all,subset=week<20,family="quasipoisson")
  plot(fit,main="GAM fit for pooled weeks <20 with s(Essential) + nbhd indicators")
  fit<-gam(formula=count~s(Essential)+s(week)+I(factor(nbhdcode))+offset(log(base)),data=dat.all,subset=week>=20&week<35,family="quasipoisson")
  plot(fit,main="GAM fit for pooled 20 <= week <35 with s(Essential) + nbhd indicators")
  fit<-gam(formula=count~s(Essential)+s(week)+I(factor(nbhdcode))+offset(log(base)),data=dat.all,subset=week>=35,family="quasipoisson")
  plot(fit,main="GAM fit for pooled weeks 35+ with s(Essential) + nbhd indicators")
}
```


```{r load-data-2, echo=F, message=FALSE, warning=FALSE, include=DRAFT}
  MI.M <- 20 # number of imputes
  datOrig <- as_tibble(read_stata("../Data/London/NewWeek12t126.dta")) 
  datCore <- as_tibble(read_stata("../Data/London/NewWeek12t76_imputedFu0.dta")) %>% filter(imputeNumber==1) %>% select(-newCases,-imputeNumber)
  
  #correlation plot
  demog.London <- datCore %>% filter(week==15) %>% mutate(GROUP=MSOA11CD,Essential=100*proportion_at_risk_jobs,BAplus=BAp,PercentOlder=100*Age65plus/AllAges,PercentYounger=100*Age015/AllAges,COPD=100*Asthma,Obese=Obesity18,PercentWhite=100-100*all_bame_prop,PercentHispanic=100*pakistani_or_bangladeshi_prop,PercentBlack=BK,OnlyEngAtHome.acs=100-FoLa,MedianInc=MedInc/1000) %>% mutate(BOROUGH_GROUP=case_when(borocode %in% c("07","20","19","22","28","33")~"Central", borocode %in% c("03","10","14")~"North", borocode %in% c("06","08","21","24","29","32")~"South", borocode %in% c("05", "09", "13","15","17","18","27")~"West", TRUE~"East")) %>% select(GROUP,BOROUGH_GROUP, Essential,BAplus,PercentOlder,PercentYounger, COPD,Obese,OnlyEngAtHome.acs,PercentWhite,PercentHispanic,PercentBlack,MedianInc) 
#log as requested
demog.London <- takeLog(demog.London,logVarNames)

names(demog.London) <-  gsub(".acs","",names(demog.London))
st(demog.London,out="latex",file="demog.London.table.tex")


ggpairs(data=demog.London,columns=3:13,aes(colour=BOROUGH_GROUP),lower = list(continuous = wrap(ggally_points, size = .1)),upper=list(continuous=wrap(ggally_cor,cex=1.4)),diag = list(continuous = wrap("densityDiag", alpha = 0.5)),title = "Correlation matrix: London")+theme_grey(base_size = 5)
  origLastWeek <- max(datCore$week)
  newLastWeek <- max(datOrig$week)
  datImp <- as_tibble(read_stata("../Data/London/NewWeek12t126_imputed0.dta")) %>% select(LaCode,MSOA11CD,week,newCases,imputeNumber)
  datLastWeek <- datCore %>% filter(week==origLastWeek) #one full record
  for (i in (1+origLastWeek):newLastWeek) {
      datCore <- add_row(datCore, (datLastWeek%>%mutate(week=i)))
  }
  dat <- datCore %>% mutate(imputeNumber=1) %>% left_join(datImp%>%filter(imputeNumber==1))
  dat <- dat %>% add_row(datCore %>% mutate(imputeNumber=2) %>% left_join(datImp%>%filter(imputeNumber==2)))
  if (MI.M>2) {
    for (i in seq(2,MI.M-2,2)) {
      datImp <- as_tibble(read_stata(paste0("../Data/London/NewWeek12t126_imputed",i,".dta"))) %>% select(LaCode,MSOA11CD,week,newCases,imputeNumber)
      dat <- dat %>% add_row(datCore %>% mutate(imputeNumber=1+i) %>% left_join(datImp%>%filter(imputeNumber==1+i)))
      dat <- dat %>% add_row(datCore %>% mutate(imputeNumber=2+i) %>% left_join(datImp%>%filter(imputeNumber==2+i)))
    }
  }
  dat <- dat %>% mutate(borocode=case_when(borocode %in% c("07","20","19","22","28","33")~1, borocode %in% c("03","10","14")~3, borocode %in% c("06","08","21","24","29","32")~4, borocode %in% c("05", "09", "13","15","17","18","27")~5, TRUE~2), base=AllAges, count=as.integer(newCases), over_65_prop= Age65plus/AllAges, under_16_prop= Age015/AllAges) %>% filter(LaCode!="")
  dat$OnlyEngAtHome.acs <- 100-dat$FoLa
  dat$PercentWhite <- 1-dat$all_bame_prop
  dat$Latinx <- dat$pakistani_or_bangladeshi_prop 
```

```{r london-vax, include=F, echo=F, warning=F,message=F}
#LONDON  - EARLIER to handle MI part

weeklyVaxDat <- as_tibble(haven::read_stata ("../Data/London/vax60-101.dta")) %>% select(MSOA11CD,week,PERC_1PLUS=percent1p) %>% mutate(logPctVaxd=log(PERC_1PLUS)) #weeklyVaxDat

firstVaxWeek <- 53
mnWeek <- min(weeklyVaxDat$week)
singleVaxDat <- as_tibble(weeklyVaxDat) %>% filter(week==mnWeek) %>% select(MSOA11CD,week,PERC_1PLUS)

#use firstVaxDat to interpolate to 2021-01-03 (week 53), when it first was effectively non-0.
for (i in (mnWeek-1):firstVaxWeek) {
    weeklyVaxDat <- weeklyVaxDat %>% add_row(singleVaxDat %>% mutate(week=i,logPctVaxd=log(PERC_1PLUS*(1+i-firstVaxWeek)/(1+mnWeek-firstVaxWeek))))
}

firstLondonWeek <- 12
for (i in (firstVaxWeek-1):firstLondonWeek) {
  weeklyVaxDat <- weeklyVaxDat %>% add_row(singleVaxDat %>% mutate(week=i,PERC_1PLUS=NA,logPctVaxd=NA))
}
weeklyVaxDat <- weeklyVaxDat %>% select(-PERC_1PLUS) %>% arrange(MSOA11CD,week)
```


```{r time-series-coef-2,  include=T, echo=F, warning=F,message=F}
  nWeeks.UK <- nWeeks <- sum(!is.na(unique(dat$week))) #toss out the NAs - KLUDGE
  varNames.UK <- c("proportion_at_risk_jobs","BAp","over_65_prop","under_16_prop","Asthma", "Obesity18", "OnlyEngAtHome.acs", "PercentWhite", "Latinx","BK", "MedInc") #names of key variables for regression 
  nVars <- length(varNames.UK)
  #create common varnames across datasets - make the proportions percents
  for (k in 1:nVars) {
    dat[varNames.US[k]] <- dat[varNames.UK[k]]*ifelse(varNames.US[k]%in%c("COPD","BAplus","Obese","OnlyEngAtHome.acs", "PercentBlack","MedianInc"),1,100) # some need *100  - but not all...
  } 
  #adjust MedInc:
  dat["MedInc"] <- dat["MedInc"]/1000 #income in 1000s
  dat["MedianInc"] <- dat["MedianInc"]/1000 #income in 1000s - should not be needed
  
  #log as requested
  dat <- takeLog(dat,logVarNames)  
  
  #now we can use a common set of varnames
  #initialize lists to store regression runs 
  #now we can add vaxx:
  varNames <- c(varNames,"logPctVaxd") #may need to reset this...
  nVars <- nVars+1
  dat <- dat %>% left_join(weeklyVaxDat,by=c("MSOA11CD","week"))
  seList <- betaList <- vector("list",nVars)
  betaList[1:nVars]<-rep(NA,nWeeks)
  seList[1:nVars]<-rep(NA,nWeeks)
  names(betaList) <- names(seList) <- varNames[1:nVars] #correction to handle lack of COPD in UK data

  weekOffset <- 11 #necessary to align the weeks as closely as possible
  #no speedup for this part (redoVarNames)... sorry
  if (readFromFile %in% c("NYC","NEITHER")) {
    cat("Imputing week:")
    for (i in 1:nWeeks) {
      cat(paste0(i,"."))
      dat.last <- dat%>%filter(week==i+weekOffset) 
      for (k in 1:nVars) {
        fmla <- as.formula(paste0("count~",varNames[k],"+factor(LaCode)+offset(log(base))")) 
        if (MI.M>0) {
          if (!all(is.na(dat.last[varNames[k]]))) {
            fit <- fitMIqpois(fmla,dat.last,varNames[k],MI.M)
            seList[[k]][i] <- fit$se
            betaList[[k]][i] <- fit$beta
          } else { #should only happen with vaxx
            seList[[k]][i] <- NA
            betaList[[k]][i] <- NA
          }
        } else {
            fit<-glm(formula=fmla,data=dat.last,family="quasipoisson"  )
            seList[[k]][i] <- summary(fit)$coef[varNames[k],"Std. Error"]
            betaList[[k]][i] <- fit$coefficients[varNames[k]]
        }
      }
    }   
    if (rescale) {
      for (i in 1:nWeeks) {
        dat.last <- dat%>%filter(week==i+weekOffset) 
        sds <- sapply(dat.last[,varNames],sd)
        for (k in 1:nVars) {
          seList[[k]][i] <- seList[[k]][i]*sds[k]
          betaList[[k]][i] <- betaList[[k]][i]*sds[k]
        }
      }
    }
    cat("\n\n")
    save(betaList,seList,file="../Data/London/betaList_seList.Rdata")
  } else load(file="../Data/London/betaList_seList.Rdata")
  dateLabels <- format.Date(as.Date(as.Date("2020-03-22")+7*(0:(nWeeks-1))),"%d%b") 
  #dateLabels <- format.Date(sort(unique(dat$date3)),"%d%b")  #problematic with missing dates
  rsltList$LON <- list(betaList=betaList,seList=seList,dateLabels=dateLabels,weekNums=1:nWeeks)
  #reset varNames, nVars for next imputes:
  varNames <- varNames[1:(length(varNames)-1)] 
  nVars <- nVars-1
``` 

```{r mlm-2, include=T, echo=F, warning=F,message=F,eval=evalFlag}
#keyIntervals/nIntervals defined in NYC portion
numCores <- parallel::detectCores()
cl <- parallel::makeCluster(numCores, setup_strategy = "sequential") 
registerDoParallel(cl)
names(mlm.fit) <- names(keyIntervals)
estTbl <- matrix(NA,nVars,nIntervals)
seTbl <- matrix(NA,nVars,nIntervals)
dimnames(seTbl) <- dimnames(estTbl) <- list(varNames, names(keyIntervals))
residList.lon <- vector("list",length=nIntervals)
names(residList.lon) <- names(keyIntervals)
fnPrefix <- paste0("est_se_tbl",ifelse(stdModel[2],"","_LAfe"))
if (readFromFile %in% c("NYC","NEITHER")) {
  cat(paste0("MI and MLM may take time (in parallel, ~1-2 hours for stdModel, 8x more for LAfe...please be patient\n",date(),'\n'))
  mdlCore <- ifelse(stdModel[2],"+factor(borocode)+poly(week,3)+offset(log(base))+(1|LaCode/MSOA11CD)","+factor(LaCode)+poly(week,3)+offset(log(base))+(1|MSOA11CD)") #two different model types
  idx <- cbind(rep(1:nIntervals,each=nVars),rep(1:nVars,times=nIntervals))
  residList <- vector("list",length=nIntervals*nVars)
  names(residList) <- paste0("[",apply(idx,1,function(x) paste0(x,collapse=",")),"]")
  #kludge to allow partial re-runs
  if (!is.null(redoVarNames)) {
    #load data, then limit runs
    load(file=paste0("../Data/London/",fnPrefix,".Rdata"))
    redoIdx <- match(redoVarNames,varNames)
    idx.hold <- idx
    idx <- idx[idx[,2]%in%redoIdx,] #subset
    #load fits 
    load(file=paste0("../Debug/fitList_Lon.Rdata"))
    fit.hold <- fit #will need to slide in some new fits
  }

  #repeat looping to exploit foreach
  fit <- foreach(i=1:nrow(idx)) %dopar% fitMImepois((fmla<-as.formula(paste0("count~",varNames[idx[i,2]],mdlCore))),subDat <- dat[dat$week>=keyIntervals[[idx[i,1]]][1]+weekOffset & dat$week<=keyIntervals[[idx[i,1]]][2]+weekOffset,],varNames[idx[i,2]],MI.M/mimFactor)
  #clean up "fit" list
  if (!is.null(redoVarNames)) {
    ct <- 1
    for (i in 1:nrow(idx.hold)) { #clean up
      if (idx.hold[i,2] %in% redoIdx) {
        fit.hold[[i]] <- fit[[ct]]
        ct <- ct + 1
      }
    } #now fit.hold is 'correct' wrt new runs   
    fit <- fit.hold
    idx <- idx.hold
  }
  for (i in 1:nrow(idx)) { #clean up
    seTbl[idx[i,2],idx[i,1]] <- fit[[i]]$se
    estTbl[idx[i,2],idx[i,1]] <- fit[[i]]$beta
    residList[[i]] <- fit[[i]]$resids 
  }
  save(estTbl,seTbl,file=paste0("../Data/London/",fnPrefix,".Rdata"))
  save(fit,file=paste0("../Debug/fitList_Lon.Rdata"))
  for (i in 1:nIntervals) {
    obsDat <- dat[dat$week>=keyIntervals[[i]][1]+weekOffset & dat$week<=keyIntervals[[i]][2]+weekOffset,] %>% filter(imputeNumber==1 & !is.na(count)) %>% select(MSOA11CD, count, week) #there remain a few missings even after impute (probably if totaled count exceeds LA count)
    newList <- residList[which(i==idx[,1])]
    newList <- lapply(newList,rlist::list.cbind) #still a list
    resids <- vector("list",length(newList))
    for (k in seq_along(newList)) {
      resids[[k]] <- data.frame(newList[[k]])
      names(resids[[k]]) <- paste0("Res_Imp",1:(MI.M/mimFactor))
      resids[[k]] <- add_column(obsDat,resids[[k]])
    }
    residList.lon[[i]] <- resids
    save(resids,file=paste0("../Data/London/",fnPrefix,"_residsPeriod",i,".Rdata"))
  }
  stopCluster(cl)
  cat(date(),'\n')
} else {
  load(file=paste0("../Data/London/",fnPrefix,".Rdata"))
  for (i in 1:nIntervals) { 
    load(file=paste0("../Data/London/",fnPrefix,"_residsPeriod",i,".Rdata"))
    residList.lon[[i]] <- resids
    names(residList.lon[[i]]) <- varNames
  }
}
if (rescale) {
  for (i in 1:nIntervals) {
      subDat <- dat[dat$week>=keyIntervals[[i]][1]+weekOffset & dat$week<=keyIntervals[[i]][2]+weekOffset,]
      sds <- sapply(subDat[,varNames],sd)
      seTbl[,i] <- seTbl[,i]*sds
      estTbl[,i] <- estTbl[,i]*sds
  }
}
#remove after rerunning NYC models
#colnames(estTbl)[colnames(estTbl)=="earlysummer21"] <- "summer21"
#colnames(seTbl)[colnames(seTbl)=="earlysummer21"] <- "summer21"
colnames(estTbl)[colnames(estTbl)=="spring23"] <- "spring22"
colnames(seTbl)[colnames(seTbl)=="spring23"] <- "spring22"
#update
estTbl.lon <- estTbl
seTbl.lon <- seTbl
```

```{r nyc-lon-coef-comp-0, include=F, echo=F, warning=F,message=F,eval=evalFlag}
lmList <-vector("list",nIntervals)
lmList.unwtd <-vector("list",nIntervals)
intRatio <- c()
pvalSlope <- vector("numeric",nIntervals)
for(i in 1:nIntervals) {
  y <- estTbl.lon[,i]
  x <- estTbl.nyc[,i]
  w <- seTbl.nyc[,i]^2
  lmList[[i]] <- lm(y~x,weights=1/w) #WLS, asm London 'precise enough'
  lmList.unwtd[[i]] <- lm(y~x) #OLS
  int1 <- predict(lmList[[i]],newdata=data.frame(x=0),interval="confidence",level = 0.95)
  int2 <- predict(lmList.unwtd[[i]],newdata=data.frame(x=0),interval="confidence",level = 0.95)
  intRatio[i] <- diff(int1[1,])[2]/diff(int2[1,])[2]
  fstat <- summary(lmList[[i]])$fstatistic
  pvalSlope[i] <- 1-pf(fstat[1],fstat[2],fstat[3]) 
}
```


```{r processResids-lon, include=F, echo=F, warning=F,message=F,eval=evalFlag}
zDat <- read_stata("../Data/London/MsoaDist.dta") # MSOA Centroids as x,y
zDat <- zDat %>% rename(lon=x,lat=y,GROUP=MSOA11CD) %>% select(lat,lon,GROUP)
#KLUDGE: if the imputes somehow reduce the MSOA count (happened in t124 run), reduce dist matrix here.
###zDat <- zDat %>% filter(GROUP %in% unique((datOrig%>%filter(LaName!=""))$MSOA11CD)) # due to problem with impute and data t124 that had to have 66 MSOAs removed!

spatTestList <- vector("list",nIntervals)
names(spatTestList) <- names(keyIntervals)
for (i in 1:nIntervals) {
  spatTestList[[i]] <- vector("list",nVars)
  names(spatTestList[[i]]) <- varNames
  spatTestList[[i]]$sigCount <- 0
  #preprocess residList.lon
  # residList.lon format:  MSOA11CD  count  week Res_Imp1 Res_Imp2 ...
  residListOne <- residList.lon[[i]]
  for (k in 1:nVars) {
    residListOne[[k]] <- residListOne[[k]] %>% select(-count) %>% rename(GROUP=MSOA11CD) %>% mutate(resid = rowMeans(select(., starts_with("Res_Imp")), na.rm = TRUE)) %>% select(GROUP,week,resid)
  }
  #preprocess:
  locs <- zDat %>% arrange(GROUP,lon,lat) 
  # NOW IT'S BACK, so remove this...%>% filter(GROUP!="E02000001") #need to deal with City of London --> drop for now

  dists <- as.matrix(dist(locs[,c("lon","lat")]))
  dists.inv <- 1/dists
  diag(dists.inv) <- 0
  for (k in 1:nVars) {
    residsOne <- residListOne[[k]]
    residsSel <- tapply(residsOne$resid,residsOne$GROUP,mean) #take mean
    #residsSel <- tapply(residsOne$resid,residsOne$GROUP,"[",1) #take first -- could be a problem, because first is only one WEEK in the interval (and potentially not the same week, though no reason for that)!
    spatTestList[[i]][[k]]$moranI <- ape::Moran.I(residsSel,dists.inv)
    spatTestList[[i]]$sigCount <- spatTestList[[i]]$sigCount + ifelse(spatTestList[[i]][[k]]$moranI$p.value < 0.05,1,0) #tally sigs
    nn1 <- knearneigh1(dist(as.matrix(locs[,c("lat","lon")])))
    nn1cor <- cor(residsSel,residsSel[nn1])
    spatTestList[[i]][[k]]$nn1cor <- nn1cor
  }
  spatTestList[[i]]$meanCor <- mean(unlist(lapply(spatTestList[[i]][1:nVars], "[","nn1cor")))
} 
spatTestList.lon <- spatTestList
desEff.lon <- matrix(NA,nVars,nIntervals)
for (i in 1:nIntervals) desEff.lon[,i] <- sqrt(1+unlist(lapply(spatTestList.lon[[i]],"[","nn1cor")))[1:nVars]
```

```{r coefplt-3, results=F,eval=evalFlag, echo=F, fig.dim=c(6,8), fig.cap="Adjusted Regression Coefficients: London", message=FALSE, warning=FALSE, include=T}
cols <- c("green","blue","red","cyan","orange","maroon","grey","purple") #have one more color than needed (I hope)
#plot the 3rd from past first!
offsets <- c(0.1*((nIntervals-4):1),NA,-0.1,-0.2,-0.3)
sigEffs.lon <- !CIcont0(estTbl,desEff.lon*seTbl)
pvals.lon <- pvalCoef(estTbl.lon,desEff.lon*seTbl.lon)
gg<-arm::coefplot(estTbl[,nIntervals-3],desEff.lon[,nIntervals-3]*seTbl[,nIntervals-3],varnames=varNames.short,col=cols[nIntervals-3],xlim=c(-0.025,0.025)*ifelse(rescale,10,1),plot=T,cex.var=.75)
for (i in (nIntervals:1)[-4]) {
 gg<-gg+arm::coefplot(estTbl[,i],desEff.lon[,i]*seTbl[,i],add=T,col=cols[i],offset=offsets[i],varnames=varNames.short)
}
print(gg)
```

```{r signifSmry-2, include=DRAFT, echo=F, warning=F,message=F,eval=evalFlag}
apply(sigEffs.nyc,2,mean)
apply(apply(pvals.nyc,2,p.adjust,method="BH")<.05,2,mean)
apply(sigEffs.lon,2,mean)
apply(apply(pvals.lon,2,p.adjust,method="BH")<.05,2,mean)
```


```{r nyc-lon-coef-comp-1, include=T, echo=F, fig.cap="Relationship between regression coefficients: NYC and London",warning=F,message=F,eval=evalFlag}
#now a graphic
estTbl0 <- estTbl.nyc
rownames(estTbl0) <- c("Ess.","BA+","Older","Young","COPD","Obese","EngOnly","White","Lat/BP","Black","Inc.")
compDat <- reshape2::melt(estTbl0)
names(compDat) <- c("variable.name","Period","NYC.coef")
lonDat <- reshape2::melt(estTbl.lon)
compDat$London.coef <- lonDat$value
sigEff.both <- reshape2::melt(sigEffs.lon*2+sigEffs.nyc+1)
compDat$Significant <- factor(sigEff.both$value,levels=1:4,labels=c("Neither","NYC","London","Both"))
gg <- ggplot(compDat,aes(x=NYC.coef,y=London.coef,label=variable.name,color=Significant,shape=Significant))+geom_point(size=.75,stroke=1.0)+scale_shape_manual(values=c(1,19,3,19)) + 
 scale_color_manual(values=c("grey10","blue","red","black"))+
  geom_text_repel(size=2.5,max.overlaps=20,show_guide=FALSE)+geom_vline(xintercept=0,color="darkgrey")+geom_hline(yintercept=0,color="darkgrey")+
  geom_smooth(data=within(compDat,Significant<-"Both"),aes(x=NYC.coef,y=London.coef),method="lm",se=T,size=.5,show_guide=FALSE)+ facet_wrap(~Period, ncol=4, nrow=2)
print(gg)
#+theme(legend.position = c(0.875, 0.2))
```



```{r des-eff,include=DRAFT,warning=F,message=F,eval=evalFlag}
par(mfrow=c(1,2))
matplot(desEff.lon,type='p',ylab='Design Effect',lwd=1,cex=.7,ylim=c(1.0,1.33),axes=F,main="London")
matlines(desEff.lon,lwd=1,lty=1)
axis(2);axis(1,labels=varNames,at=1:nVars,pos=1,las=2,cex.axis=.5)
matplot(desEff.nyc,type='p',ylab='Design Effect',lwd=1,cex=.7,ylim=c(1.0,1.33),axes=F,main="NYC")
matlines(desEff.nyc,lwd=1,lty=1)
axis(2);axis(1,labels=varNames,at=1:nVars,pos=1,las=2,cex.axis=.5)
```

```{r plot-ts-prelim-NYC-1, include=T, echo=F, warning=F,message=F}
#get time series aggregated
datOne <- as.Date("2020-03-22")

require(readr)
urlfile<-"https://raw.githubusercontent.com/nychealth/coronavirus-data/master/trends/cases-by-day.csv"
ts.agg.nyc <-read_csv(url(urlfile))
ts.agg.nyc.cases <- ts.agg.nyc %>% mutate(date=format(as.Date(date_of_interest , '%m/%d/%Y'), '%Y-%m-%d')) %>% select(date,Cases7Day=CASE_COUNT_7DAY_AVG)
#ts.agg.nyc <- read.csv("../Data/NYC/cases-by-day.csv",header=T)
ts.agg.nyc$date_of_interest <- format(as.Date(ts.agg.nyc$date_of_interest , '%m/%d/%Y'), '%Y-%m-%d')
ts.agg.nyc$day <- as.integer(as.Date(ts.agg.nyc$date_of_interest)-datOne)
ts.agg.nyc <- as_tibble(ts.agg.nyc) %>% rename(date=date_of_interest,total_count=ALL_CASE_COUNT_7DAY_AVG) %>% filter(day>=0 & day%%7==0) %>% mutate(wkNums=1+floor(day/7),total_count=7*total_count) %>% select(date,wkNums,total_count) #weekly not daily count
#scale to 0-1
mx.ts <- max(ts.agg.nyc$total_count)
mn.ts <- min(ts.agg.nyc$total_count)
ts.agg.nyc <- ts.agg.nyc %>% mutate(scaled_count=(total_count-mn.ts)/(mx.ts-mn.ts))
```

```{r plot-ts-prelim-NYC-2, include=DRAFT, echo=F, warning=F,message=F}
require(readr)
urlfile<-"https://raw.githubusercontent.com/nychealth/coronavirus-data/master/trends/hosp-by-day.csv"
ts.agg.nyc.hosps <-read_csv(url(urlfile))
ts.agg.nyc.hosps <- ts.agg.nyc.hosps %>% mutate(date=format(as.Date(date_of_interest , '%m/%d/%Y'), '%Y-%m-%d')) %>% select(date,Hosps7Day=HOSP_COUNT_7DAY_AVG)
ts.agg.nyc.comps <- ts.agg.nyc.cases %>% left_join(ts.agg.nyc.hosps)
ts.agg.nyc.comps$day <- as.integer(as.Date(ts.agg.nyc.comps$date)-datOne)
ts.agg.nyc.comps <- ts.agg.nyc.comps %>% mutate(week=1+floor(day/7))
ggplot(data=ts.agg.nyc.comps%>% filter(day%%7==0,week>0),aes(y=Hosps7Day/Cases7Day,x=week))+geom_point()+ggtitle("NYC: Relationship between hospitalizations and cases detected, over time")
```


```{r comp-adj-constructed, echo=F, message=FALSE, warning=FALSE, include=DRAFT}
ts.agg.nyc.compare <- tapply(dat.all$count,dat.all$week,sum,na.rm=T)
ts.agg.nyc.compare[1] <- ts.agg.nyc.compare[1]/8 #correction 
#ts.agg.nyc.compare<- ts.agg.nyc.compare[ 1:(length(ts.agg.nyc.compare)-1)]
ts.plot(ts.agg.nyc.compare,main="Compare Adjusted (red) to constructed NYC series",ylab="Weekly Count")
from7day <- 7*(ts.agg.nyc.cases %>% filter(date>="2020-05-18",as.numeric((as.Date(date)-as.Date("2020-05-18"))) %% 7 ==0) %>% select(Cases7Day) %>% unlist())
lines((1:length(from7day)),from7day,col=2)
```

```{r plot-ts-prelim-Lon-1, include=T, echo=F, warning=F,message=F}
#get time series aggregated
ts.agg.lon <- dat %>% group_by(week) %>% summarise(total_count=sum(count,na.rm=T)/MI.M)

#adjust first 9 weeks by NYC ratios:
ts.adj <- ts.agg.nyc.comps%>% filter(day%%7==0,week>0,week<10) %>% mutate(adjRatio=Cases7Day/Hosps7Day,week=week+weekOffset) %>% select(week,adjRatio)
ts.agg.lon <- ts.agg.lon %>% left_join(ts.adj) 
ts.agg.lon$adjRatio[is.na(ts.agg.lon$adjRatio)] <- 1
ts.agg.lon <- ts.agg.lon %>% mutate(total_count = total_count*adjRatio)
#scale to 0-1
mx.ts <- max(ts.agg.lon$total_count)
mn.ts <- min(ts.agg.lon$total_count)
ts.agg.lon<- ts.agg.lon %>% mutate(wkNums=week-weekOffset, scaled_count=(total_count-mn.ts)/(mx.ts-mn.ts)) %>% select(wkNums,total_count,scaled_count)
```

```{r plot-ts-base-both-1, include=T, echo=F, warning=F,message=F}
ts.dat <- ts.agg.nyc %>% mutate(loc="NYC") %>% select(wkNums,total_count,loc)
  ts.dat <- add_row(ts.dat,ts.agg.lon %>% mutate(loc="London") %>% select(wkNums,total_count,loc))
  gg_base0 <- ggplot(data=ts.dat,aes(x=wkNums,y=total_count/1000,color=loc))+geom_line(data=ts.dat,stat="smooth",method = "loess", span=.125,formula=y~x)+
    scale_color_manual(values=c("blue","red"))+theme(legend.position = "none")+ylab("")+ theme(axis.title.x=element_blank(),axis.text.x=element_blank(), axis.ticks.x=element_blank())+geom_vline(xintercept = unlist(lapply(keyIntervals,"[",1))[-1],col="darkgrey",linetype="dashed")
  gg_base <- gg_base0 + scale_y_continuous("",breaks=c(0,20,40,60,80), labels=paste0(c(0,20,40,60,80),"K"))+coord_cartesian(ylim=c(-1,87))
```


```{r plot-ts-both-1, echo=F, message=FALSE, warning=FALSE, include=DRAFT}
baseR <- F
superImpose<- F
diffWeekNumsBeg <- min(rsltList$NYC$weekNums)-min(rsltList$LON$weekNums)
diffWeekNumsEnd <- max(rsltList$NYC$weekNums)-max(rsltList$LON$weekNums)

# now we can say we have one more var to plot
nVars <- length(rsltList$NYC$betaList)
varNamesNew <- c(varNames,"logPctVaxd")
#use "reorder", based on prefs:
if (is.null(tsOrder)) tsOrder <- 1:nVars
pltDat.all <- NULL
for (k in tsOrder) {
 uppBd.lon <- rsltList$LON$betaList[[k]]+1.96*rsltList$LON$seList[[k]]
 lowBd.lon <- rsltList$LON$betaList[[k]]-1.96*rsltList$LON$seList[[k]]
 uppBd.nyc <- rsltList$NYC$betaList[[k]]+1.96*rsltList$NYC$seList[[k]]
 lowBd.nyc <- rsltList$NYC$betaList[[k]]-1.96*rsltList$NYC$seList[[k]]
 nWeeks <- max(nWeeks.US,nWeeks.UK)
 lonLines <- cbind(lowBd.lon,rsltList$LON$betaList[[k]],uppBd.lon)
 wkNums <- rsltList$LON$weekNums
 axLabels <- rsltList$LON$dateLabels
 if (diffWeekNumsEnd>0) { #more NYC weeks
   lonLines <- rbind(lonLines,matrix(NA,diffWeekNumsEnd,dim(lonLines)[2]))
   wkNums <- c(wkNums, max(wkNums)+(1:diffWeekNumsEnd)) 
   axLabels <- format.Date(as.Date(as.Date("2020-03-22")+7*(wkNums-1)),"%d%b") 
 } 
 
 #correction for plotting -- zero out the early missing period.
 rsltList$LON$betaList[["logPctVaxd"]][is.na(rsltList$LON$betaList[["logPctVaxd"]]) & rsltList$LON$weekNums < 70] <- 0 #70 is too large but a safe choice to prevent last values from being changed
 rsltList$NYC$betaList[["logPctVaxd"]][is.na(rsltList$NYC$betaList[["logPctVaxd"]])] <- 0
 
 nycLines <- cbind(lowBd.nyc,rsltList$NYC$betaList[[k]],uppBd.nyc)
 nycLines<- rbind(matrix(NA,diffWeekNumsBeg,dim(nycLines)[2]),nycLines)
 
 if (diffWeekNumsEnd<0) { #more Lon weeks
   nycLines <- rbind(nycLines,matrix(NA,-diffWeekNumsEnd,dim(nycLines)[2]))
 }
 
 if (baseR) {
   ts.plot(lonLines,col=c("lightblue","blue","lightblue"),xlab="",gpars= list(xaxt="n"),main=varNamesNew[k],xlim=c(min(wkNums),max(wkNums)),ylim=c(min(c(lowBd.lon,lowBd.nyc)),max(c(uppBd.lon,uppBd.nyc))))
    legend("topright",lty=c(1,1),col=c("blue","red"),legend=c("London","NYC"),cex=.8,bty='n')
    axis(1,at=wkNums,labels=axLabels,las=2,cex.axis=.6);abline(h=0,col=8)
    #need to 'pad' nyc lines by the difference in starting points, LON, NYC
    matlines(wkNums,nycLines,col=c("pink","red","pink"),lty=c(1,1,1))#when LON & NYC not aligned... use LON as x-axis basis
 } else {
  ticks <- wkNums[c(T,F,F)] #every 3
  pltDatWide <- as.data.frame(cbind(nycLines,lonLines)) %>% rename(beta.nyc=V2,beta.lon=V5) %>% add_column(wkNums,axLabels)
  pltDat <- reshape::melt(pltDatWide,id=c("wkNums","axLabels")) %>% filter(variable != "NA",!is.na(value)) %>% mutate(bound=grepl("Bd",variable),city=ifelse(grepl("nyc",variable),"NYC","London"))
  #to see London shifted forward to coincide more with NYC:
  #pltDat$wkNums[pltDat$city=="London"] <- pltDat$wkNums[pltDat$city=="London"]+4
  #Do not use the above, but consider it as an omnibus test (?)
  maxHt <- max(pltDat$value)
  minHt <- min(pltDat$value)
  pltDat <- pltDat %>% left_join(ts.agg.nyc %>% mutate(tot_ct_nyc=scaled_count*(maxHt-minHt)+minHt) %>% select(wkNums,tot_ct_nyc)) %>% left_join(ts.agg.lon %>% mutate(tot_ct_lon=scaled_count*(maxHt-minHt)+minHt) %>% select(wkNums,tot_ct_lon))
  gg<-ggplot(data=pltDat,aes(x=wkNums,y=value,group=variable,color=city,linetype=factor(bound)))+geom_line(data=pltDat,stat="smooth",method = "loess", span=.25,formula=y~x)+ stat_smooth(se=F,span=.25,size=.5)+geom_hline(yintercept=0)+scale_color_manual(values=c("blue","red"))+scale_linetype_manual(values=c("solid","dotted"))+ theme(legend.position = "bottom")+guides(linetype=FALSE)+ labs(color="",x="",y="Coefficient")+scale_x_continuous(breaks = ticks, 
                 labels = axLabels[ticks])+theme(axis.text.x=element_text(angle = 90))+labs(title=element_text(varNamesNew[k]))+geom_vline(xintercept=unlist(lapply(keyIntervals,"[",1))[-1],col="darkgrey",linetype="dashed")
 if (superImpose) {                   
   gg <- gg+geom_line(data=pltDat,stat="smooth",method = "loess", span=.15, formula=y~x,color="pink",aes(x=wkNums,y=tot_ct_nyc))+annotate(geom="text", x=43+4, y=maxHt+.01, label="NYC Cases", color="red",alpha=.4)
   gg_final <- gg+geom_line(data=pltDat,stat="smooth",method = "loess", span=.15, formula=y~x,color="lightblue",aes(x=wkNums,y=tot_ct_lon))+annotate(geom="text", x=42-4, y=maxHt-.01, label="London Cases", color="blue",alpha=.4)
 } else {
   #grid
   gg_final <- grid.arrange( grobs = list(gg_base,gg), heights=c(1.5,7), nrow=2)
 }
  print(gg_final)
 }
 pltDat$varname <- varNamesNew[k]
 pltDat$varidx <- k
 pltDat.all <- rbind(pltDat.all,pltDat)
}
```

```{r pooled-ts-1, echo=F, message=FALSE, warning=FALSE, fig.cap="Adjusted Correlates over Time",include=T,fig.width=7,fig.height=9.5}
gg <- ggplot(data=pltDat.all%>% filter(varidx %in% tsOrder[1:3]), aes(x=wkNums,y=value,group=variable,color=city,linetype=factor(bound)))+geom_line(stat="smooth",method = "loess", span=.25,formula=y~x)+ stat_smooth(se=F,span=.25,size=.5)+geom_hline(yintercept=0)+scale_color_manual(values=c("red","blue"))+scale_linetype_manual(values=c("solid","dotted"))+ theme(legend.position = "bottom")+guides(linetype=FALSE)+scale_y_continuous(limits=c(-0.375,+0.375))+ labs(color="",x="",y="Coefficient")+scale_x_continuous(breaks = ticks, labels = axLabels[ticks])+ theme(axis.text.x=element_text(angle = 90))+geom_vline(xintercept=unlist(lapply(keyIntervals,"[",1))[-1],col="darkgrey",linetype="dashed")+ facet_grid(facet=varname~.,scales="free") 

#alter data
gg_base0$data <- transform(gg_base0$data,label="Incidence")
gg_base <- gg_base0 + scale_y_continuous("",breaks=c(0,25,50,75,100,125,150), labels=paste0(c(0,25,50,75,100,125,150),"K"))+coord_cartesian(ylim=c(-1,151))+facet_grid(facet=label~.)+scale_color_manual(values=c("red","blue"))
grid.arrange( grobs = list(gg_base,gg), heights=c(3,12), nrow=2)
```                 

```{r anim-ts-1, echo=F, message=FALSE, warning=FALSE, include=DRAFT,results='asis'}
week1nyc <- min(rsltList$NYC$weekNums)
weekBeg <- max(week1nyc,min(rsltList$LON$weekNums)) #limit to overlapping weeks, NYC, London
weekEnd <- min(max(rsltList$NYC$weekNums),max(rsltList$LON$weekNums)) #limit to overlapping weeks, NYC, London
nTicks <- 7
rng <- seq(-.3,.3,length=nTicks)
cols <- rainbow(weekEnd-weekBeg)
oldpar<-par()
par(mfrow=c(2,2),plt=c(.15,.85,.15,.85))  
for (i in seq_along(rsltList$NYC$betaList)) {
  plot(rng,rng,type='n',xlab="",ylab="",axes=F)
  title(main=names(rsltList$NYC$betaList)[i],cex.main=.7,line=0.25)
  axis(1,at=rng,label=rep("",nTicks),tck=0.02)
  axis(1,cex.axis=.7,lwd=0,line=-1)
  axis(2,at=rng,label=rep("",nTicks),tck=0.02)
  axis(2,cex.axis=.7,lwd=0,line=-1)
  mtext(side=1, line=.65, "NYC.coef",cex=.4)
  mtext(side=2, line=.65, "London.coef",cex=.4)
  abline(h=0,col=8);abline(v=0,col=8)
  
  sm.NYC <- rsltList$NYC$betaList[[i]]
  b <- !is.na(sm.NYC) #nemove NAs for Tukey smoother.
  sm.NYC[b] <- smooth(sm.NYC[b])
  sm.LON <- rsltList$LON$betaList[[i]]
  b <- !is.na(sm.LON) #nemove NAs for Tukey smoother.
  sm.LON[b] <- smooth(sm.LON[b])
  points(sm.NYC[weekBeg-week1nyc+1],sm.LON[weekBeg],pch=1,col=1,cex=1.5)
  arrows(sm.NYC[(weekBeg:(weekEnd-1))-week1nyc+1],sm.LON[weekBeg:(weekEnd-1)],sm.NYC[((weekBeg+1):weekEnd)-week1nyc+1],sm.LON[(weekBeg+1):weekEnd],length=0.025,col=cols)
  points(sm.NYC[weekEnd-week1nyc+1],sm.LON[weekEnd],pch=4,col=1,cex=1.5)
  if (i%%4==0) print("<P style='page-break-before: always'>")  
}
par(oldpar)
```

```{r NYC-age-plots, echo=FALSE, out.width='100%', include=DRAFT}
knitr::include_graphics('../Results/NYC/dailyByBoroAge.png')
```

```{r smooth-plot, echo=F, message=FALSE, warning=FALSE, include=T}
#SCRATCH - TESTS - 
#ggplot(data=pltDat,aes(x=wkNums,y=value,group=city,color=city,linetype=bound))+stat_smooth(data=pltDat,se=F,span=.25,size=.5,aes(alpha=1-1*bound/2))+geom_hline(yintercept =0)+scale_color_manual(values=c("pink","red","pink","lightblue","blue","lightblue"))+scale_linetype_manual(values=c(1,1))
#ggplot(data=pltDat,aes(x=wkNums,y=value,group=variable,color=city,linetype=bound))+stat_smooth(data=pltDat,se=F,span=.25,size=.5,aes(alpha=1-1*bound/2))+geom_hline(yintercept =0)+scale_color_manual(values=c("pink","red","pink","lightblue","blue","lightblue"))+scale_linetype_manual(values=c(1,1))
#+scale_x_continuous(breaks=1:65,labels=axLabels)
```

```{r load-data-3,  echo=F, message=FALSE, warning=FALSE, include=T}
#datage <- as_tibble(read_stata("../Data/London/age.dta"))
datage <- as_tibble(read_stata("../Data/London/agerec.dta")) %>% select(-age2) %>% rename(age2=age3) %>% mutate(age=as.character(haven::as_factor(age2))) 

datage <- datage %>% mutate(borocode=case_when(borcode %in% c("07","20","19","22","28","33")~1, borcode %in% c("02","04","11","12","16","23","25","26","30","31")~2, borcode %in% c("03","10","14")~3, borcode %in% c("06","08","21","24","29","32")~4, borcode %in% c("05", "09", "13","15","17","18","27")~5))
datage <- datage %>% mutate(LondSR=case_when(borocode %in% c(1)~"Central London",borocode %in% c(2)~"East London",borocode %in% c(3)~"North London",borocode %in% c(4)~"South London", borocode %in% c(5)~"West London")) %>% filter(!is.na(borocode))
```

```{r ggplot, fig.keep = 'all', Tidy=TRUE, echo=F, message=FALSE, warning=FALSE, include=DRAFT, out.width="120%", fig.width = 7 ,fig.height = 7,dpi=144}
dateOffset <- as.Date("2020-05-18") - as.Date(min(datage$date))
datage$date3 <- datage$date2 - min(datage$date2) - dateOffset + 1
ggplot(datage, aes(date3,y=cases*(100000/pop),group=age2,color=age)) + stat_smooth(se=FALSE, method="loess", span=.3,size=.5) + theme(legend.position="bottom")+ ylab(expression(Delta*" per 100K"))+facet_wrap(.~LondSR,nrow=2,ncol=3)+labs(title="7-day New Cases of Covid-19 in London by Age Group", caption = "Data source: ONS")+coord_cartesian(ylim=c(0, 112.5))+xlim(1,max(datage$date3))+xlab(paste0("Days since 18May20"))

#ggplot(datage, aes(week,y=cases*(100000/pop),group=age2,color=age)) + stat_smooth(se=FALSE, span=.5,size=.5) + theme(legend.position="bottom")+ ylab(expression(Delta*" per 100K"))+xlim(45,70)+facet_wrap(.~borocode,nrow=2,ncol=3)+facet_wrap(.~LondSR,nrow=2,ncol=3)+labs(title="7-day New Cases of Covid-19 in London by Age Group, Third wave", caption = "Data source: ONS")+coord_cartesian(ylim=c(0, 200))
```

```{r heat-plot-nyc, echo=F, message=FALSE, warning=FALSE, include=DRAFT}
lastWeek <- max(dat.all$week)
arrDat <- dat.all %>% select(GROUP,borocode,BOROUGH_GROUP,week,count,base,BAplus,Essential,MedianInc,PercentWhite) %>% mutate(rate=100000*count/base) %>% arrange(borocode,GROUP,week)  
arrDat$lastrate <- rep((arrDat %>% filter(week==lastWeek-0))$rate,each=lastWeek) #week 42 good 
arrDat <- arrDat %>% arrange(borocode,week,lastrate) %>% group_by(borocode,week) %>% mutate(rank=rank(PercentWhite),n=n(),frac=(rank-1)/(n-1)) #or rank(lastRate)
pg <- ggplot(data=arrDat%>% mutate(rate=pmax(0,pmin(500,rate))) %>% filter(week>=28,week<=lastWeek), aes(week, frac)) +
  facet_wrap(.~BOROUGH_GROUP,nrow=2,ncol=3) + geom_raster(aes(fill = rate)) + scale_fill_gradient2(low="white", mid="yellow", high="red", guide="colorbar", na.value="white")+theme(legend.position = c(0.85, 0.2)) #or rate=log(rate)
print(pg)
#lattice::levelplot((rate)~week*frac|BOROUGH_GROUP,col.regions=rev(heat.colors(100)),data=arrDat%>% mutate(rate=pmin(500,rate)) %>% filter(week>=28,week<=lastWeek))
```

```{r heat-plot-lon, echo=F, message=FALSE, warning=FALSE, include=DRAFT}
dat.london <- dat %>%
  select(week, date, MSOA11CD, Localauthority, pop18, borocode,
         cases, count, Essential, MedianInc, PercentWhite) %>%
  mutate(rate=100000*count/pop18, week=week-weekOffset) %>%
  group_by(borocode,MSOA11CD,week) %>% summarize(rate=mean(rate,na.rm=T), Localauthority=Localauthority,Essential=Essential, MedianInc=MedianInc, PercentWhite=PercentWhite) %>% mutate(LondSR=case_when(borocode %in% c(1)~"Central London",borocode %in% c(2)~"East London",borocode %in% c(3)~"North London",borocode %in% c(4)~"South London", borocode %in% c(5)~"West London")) 

lastWeek <- max(dat.london$week)

dat.london$lastrate <- rep((dat.london %>% filter(week==lastWeek))$rate,each=lastWeek)

arrDat <- dat.london %>% arrange(borocode,week,lastrate) %>% group_by(borocode,week) %>% mutate(rank=rank(lastrate),n=n(),frac=(rank-1)/(n-1)) #or rank(lastRate)
pg <- ggplot(data=arrDat%>% mutate(rate=pmax(0,pmin(500,rate))) %>% filter(week>=28,week<=lastWeek), aes(week, frac)) +
 facet_wrap(.~LondSR,nrow=2,ncol=3) + geom_raster(aes(fill = rate)) + scale_fill_gradient2(low="white", mid="yellow", high="red", guide="colorbar", na.value="white")+theme(legend.position = c(0.85, 0.2)) #or rate=log(rate)
print(pg)
```

